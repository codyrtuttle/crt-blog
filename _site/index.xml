<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:distill="https://distill.pub/journal/" version="2.0">
  <channel>
    <title>Cody R. Tuttle</title>
    <link>https://codyrtuttle.netlify.app</link>
    <atom:link href="https://codyrtuttle.netlify.app/index.xml" rel="self" type="application/rss+xml"/>
    <description>Cody R. Tuttle
</description>
    <generator>Distill</generator>
    <lastBuildDate>Tue, 27 Apr 2021 00:00:00 +0000</lastBuildDate>
    <item>
      <title>Birth rate analysis vol 1: Maps, maps, and more maps</title>
      <dc:creator>Cody Tuttle</dc:creator>
      <link>https://codyrtuttle.netlify.app/posts/2021-04-27-birth-rate-analysis-vol-1-maps-maps-and-more-maps</link>
      <description>


&lt;h1 id="hello"&gt;Hello!&lt;/h1&gt;
&lt;p&gt;Hey, folks! I’m happy to be back swinging some fresh data science and musings for all to see (or not see), and I’m happy you’re here. If you’re reading this, first of all, thanks. I really do appreciate it, whether you make it to all the way through the post or just to the end of this paragraph.&lt;/p&gt;
&lt;p&gt;To be honest, I’ve been in a bit of a rut as far as what to post about goes the last few weeks. After my last post, I was originally thinking that I’d explore some COVID data that the CDC released not too far back. When that turned out to be a bit too unwieldy for my poor little MacBook Air to handle, I thought about going down the housing/mortgage data route, but I realized after a bit that the questions I was interested in asking would be pretty similar to the types of questions in the CFPB post I lifted from my old blog a few months ago. Not that there’s anything wrong with that, but I wanted to broach new territory, dig into new kinds of data and ask fresh questions.&lt;/p&gt;
&lt;h1 id="birth-rates"&gt;Birth Rates!&lt;/h1&gt;
&lt;p&gt;What I finally came to, after casually asking my wife one evening what I should blog about next, was birth rates. As you might know, we’re about to welcome our first kid into the world (a baby girl!) in mid-May, so it’s obviously a topic that’s been on our minds a lot. I’d toyed with the idea of data on baby names (not super interesting to me) or maternal mortality and the egregious racial disparities therein (too heavy this close to our own birth experience), but when Mariana mentioned birth rates, it totally clicked. I started looking at some birth and fertility rates by state and year from the CDC and had the idea to join it with some data on state government spending on children from Tidy Tuesday and with some state welfare and socioeconomic data that I used for my MPP thesis. After some exploration and thinking, I decided that there was probably too much that I was interested in with these data to put it all in one post.&lt;/p&gt;
&lt;p&gt;So I decided to split this topic into three posts - First, different maps of birth rates across time; second, an exploratory visualization of the association between birth rates and socioeconomic/political state characteristics; and third, causal models on whether state spending on kids impacts birth rates over time.&lt;/p&gt;
&lt;h1 id="todays-post-maps"&gt;Today’s Post: Maps!&lt;/h1&gt;
&lt;p&gt;Y’all have probably realized this by now, but I love maps. I’m not a geographer by any means, but since mid-college I’ve loved maps and geography, and it’s been really fun to explore the different spatial and mapping capabilities in R over the last several years. &lt;code&gt;Leaflet&lt;/code&gt; is probably my favorite R package for mapping, but even &lt;code&gt;sf&lt;/code&gt; and &lt;code&gt;ggplot2&lt;/code&gt; static mapping is great. Though I know it’s not always appropriate, whenever I have any element of geography, even just US states in my data, I like to map them. Usually I like to do interactive maps, where the user can pan and zoom and all that good stuff, which is why I love &lt;code&gt;leaflet&lt;/code&gt; so much. But sometimes that much interactivity is more of a hindrance than a help - this is something I explore a bit in this post.&lt;/p&gt;
&lt;h2 id="data"&gt;Data&lt;/h2&gt;
&lt;p&gt;First things first, the data I’m using come from the &lt;a href="https://wonder.cdc.gov"&gt;CDC Wonder database&lt;/a&gt;. They have tons of data, but the data I use are obviously on birth and natality. I pulled two files from them, both containing birth and fertility rates by state and year, one from 2003-2006 and one from 2007-2019. They also have data from 1995-2002 that I looked at, but they didn’t have the same fertility rate measures. I also use the &lt;code&gt;maps&lt;/code&gt; package to pull state shapefiles, but that’s it for this post. In later posts I’ll pull in other sources, but I only use the birth rates in this one.&lt;/p&gt;
&lt;h2 id="plan"&gt;Plan&lt;/h2&gt;
&lt;p&gt;One of the things that became apparent to me while exploring different ideas for this post is that there are different levels of interactivity, and it’s important to choose the right level of interactivity for your situation. With coordinate or line spatial data or a very zoomed out map, it’s really nice to have pan and zoom interactivity, but with something as simple as US states, that’s not really necessary - a simple tooltip or hover button will do just fine, and maybe even a static map is best in that case. In this post I wanted to play around with what the right level of interactivity was for this specific use case, one simple measure with data at the state and year level.&lt;/p&gt;
&lt;p&gt;BLUF, I ended up exploring three different maps to explore the data - a faceted state map by year (no tooltip or interactivity), a state map with a drop down filter to select the year using &lt;code&gt;bsselectR&lt;/code&gt;, and last an interactive hover-tooltip map of a single year using &lt;code&gt;ggiraph&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="data-prep"&gt;Data Prep&lt;/h2&gt;
&lt;p&gt;This section will just be the code to import and clean the data - super simple. Thanks to the &lt;code&gt;maps&lt;/code&gt; package for the easy US state shapefiles!&lt;/p&gt;
&lt;p&gt;First, load necessary packages and set options:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;library(tidyverse) #loading and cleaning data
library(readr) #loading data
library(ggiraph) #interactive tooltip graphs!!!
library(janitor) #clean imported data columns
library(bsselectR) #interactive dropdown menus without Shiny!!!
library(maps) #state shapefiles
library(mapproj) #support sf
library(sf) #mapping with ggplot!
library(ggthemes) #use for theme_map()
library(glue)
library(purrr)

options(scipen = 999)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now read in the data:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# birth rate data from CDC wonder
natality0306 &amp;lt;- read_delim(&amp;quot;Natality, 2003-2006.txt&amp;quot;,&amp;quot;\t&amp;quot;, escape_double = FALSE, trim_ws = TRUE) %&amp;gt;% 
  clean_names() %&amp;gt;% 
  select(-notes)

natality0719 &amp;lt;- read_delim(&amp;quot;Natality, 2007-2019.txt&amp;quot;, &amp;quot;\t&amp;quot;, escape_double = FALSE, trim_ws = TRUE)%&amp;gt;% 
  clean_names() %&amp;gt;% 
  select(-notes)

natality &amp;lt;- bind_rows(natality0306, natality0719) %&amp;gt;% 
  filter(!is.na(state))

# state shapefiles 
states &amp;lt;- map_data(&amp;quot;state&amp;quot;)

# join natality and shapefiles
natality &amp;lt;- natality %&amp;gt;% 
  mutate(region = tolower(state))

state_year &amp;lt;- states %&amp;gt;% 
  left_join(natality, by = &amp;quot;region&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="static-state-map-faceted-by-year"&gt;Static State Map Faceted by Year&lt;/h2&gt;
&lt;p&gt;Now comes the fun - time to map!&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;walk(2003:2019, ~{
  
  gg &amp;lt;- ggplot(data = filter(state_year, year == .x),
            aes(x = long, y = lat, group = group, fill = fertility_rate, 
                tooltip = fertility_rate, 
                data_id = state)) +
  geom_polygon_interactive(color = &amp;#39;gray70&amp;#39;) +
  coord_map(projection = &amp;quot;albers&amp;quot;, lat0 = 39, lat1 = 45) +
  labs(title = glue(&amp;quot;US State Fertility Rate by State, {.x}&amp;quot;), 
       subtitle = &amp;quot;Hover over state to see fertility rate&amp;quot;) +
  theme_map()
  
  ggi &amp;lt;- girafe(ggobj = gg)
  
  htmlwidgets::saveWidget(ggi, file = glue(&amp;quot;{.x}.html&amp;quot;))
})&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="r"&gt;&lt;code&gt;plots &amp;lt;- list.files(pattern = &amp;quot;.html&amp;quot;)
names(plots) &amp;lt;- 2003:2019

bsselect(plots, type = &amp;quot;iframe&amp;quot;, selected = &amp;quot;2019&amp;quot;)&lt;/code&gt;&lt;/pre&gt;

&lt;html&gt;
&lt;select id="XPaNLQz0Ac" class="selectpicker" data-dropdown-align-right="false" data-dropup-auto="true" data-header="false" data-live-search="false" data-live-search-style="contains" data-show-tick="false" data-width="false" data-size="auto"&gt;&lt;option value="2003.html"&gt;2003&lt;/option&gt;
&lt;option value="2004.html"&gt;2004&lt;/option&gt;
&lt;option value="2005.html"&gt;2005&lt;/option&gt;
&lt;option value="2006.html"&gt;2006&lt;/option&gt;
&lt;option value="2007.html"&gt;2007&lt;/option&gt;
&lt;option value="2008.html"&gt;2008&lt;/option&gt;
&lt;option value="2009.html"&gt;2009&lt;/option&gt;
&lt;option value="2010.html"&gt;2010&lt;/option&gt;
&lt;option value="2011.html"&gt;2011&lt;/option&gt;
&lt;option value="2012.html"&gt;2012&lt;/option&gt;
&lt;option value="2013.html"&gt;2013&lt;/option&gt;
&lt;option value="2014.html"&gt;2014&lt;/option&gt;
&lt;option value="2015.html"&gt;2015&lt;/option&gt;
&lt;option value="2016.html"&gt;2016&lt;/option&gt;
&lt;option value="2017.html"&gt;2017&lt;/option&gt;
&lt;option value="2018.html"&gt;2018&lt;/option&gt;
&lt;option value="2019.html" selected&gt;2019&lt;/option&gt;
&lt;option value="birth-rate-analysis-vol-1-maps-maps-and-more-maps.html"&gt;NA&lt;/option&gt;&lt;/select&gt;
&lt;iframe src="2019.html" frameborder="0" height="500" width="100%" id="njoNGJOCcL"&gt;&lt;/iframe&gt;
&lt;script&gt;$(document).ready(function(){
                   $("#XPaNLQz0Ac").change(function(){
                   $("#njoNGJOCcL").attr("src",$(this).val());

                   });

    });&lt;/script&gt;
&lt;/html&gt;
&lt;div id="htmlwidget-4b1c9d90566ef7e6f96b" style="width:672px;height:480px;" class="bsselect html-widget" width="672" height="480"&gt;&lt;/div&gt;
&lt;script type="application/json" data-for="htmlwidget-4b1c9d90566ef7e6f96b"&gt;{"x":[],"evals":[],"jsHooks":[]}&lt;/script&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>15cee258cdc32a1265ae9573a663bb02</distill:md5>
      <category>R</category>
      <category>maps</category>
      <category>R spatial</category>
      <category>ggiraph</category>
      <category>interactivity</category>
      <guid>https://codyrtuttle.netlify.app/posts/2021-04-27-birth-rate-analysis-vol-1-maps-maps-and-more-maps</guid>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Plug for the Correlates of State Policy Shiny App</title>
      <dc:creator>Cody Tuttle</dc:creator>
      <link>https://codyrtuttle.netlify.app/posts/2021-04-12-plug-for-the-correlates-of-state-policy-shiny-app</link>
      <description>A quick shout out to a really cool and valuable tool for social scientists</description>
      <category>state policy</category>
      <category>no code</category>
      <guid>https://codyrtuttle.netlify.app/posts/2021-04-12-plug-for-the-correlates-of-state-policy-shiny-app</guid>
      <pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>New Shiny App on World Coffee Ratings!</title>
      <dc:creator>Cody Tuttle</dc:creator>
      <link>https://codyrtuttle.netlify.app/posts/2021-04-05-new-shiny-app-on-world-coffee-ratings</link>
      <description>


&lt;p&gt;Hey hey hey, folks. Hope everyone is doing swell. I wanted to take just a few minutes to do a small post about a fun little &lt;a href="https://crtut13.shinyapps.io/coffee_ratings/"&gt;Shiny web app&lt;/a&gt; I just created on world coffee ratings.&lt;/p&gt;
&lt;p&gt;Coffee, especially the specialty coffee scene, is near and dear to my heart. Back in college and for a brief period during my first master’s I worked at a local specialty coffee shop in Fayetteville, Arkansas called &lt;a href="https://onyxcoffeelab.com/"&gt;Onyx Coffee Lab&lt;/a&gt;, and it introduced me to a world I will forever appreciate. For those that have never delved into this world, coffee is a product that is similar to wine in that there is a whole world of producers, varietals, processing methods, brewing/production methods, and even competitions that accompany it. It’s not just caffeine and dark roasts, Folgers and Starbucks - there is an entire industry dedicated to finding, procuring, roasting, and brewing the best coffees in the world.&lt;/p&gt;
&lt;p&gt;This Shiny &lt;a href="https://crtut13.shinyapps.io/coffee_ratings/"&gt;app&lt;/a&gt; (and post) is an homage to that world.&lt;/p&gt;
&lt;p&gt;I was hunting through old TidyTuesday data archives and came across a week with world coffee ratings as the featured data, and I jumped on it. It’s honestly a pretty simple web app, but I think it’s really fun. A user can select from any coffee in the database and it will pull up a polar coordinate set of its ratings along a few different dimensions, including acidity, balance, sweetness, uniformity of beans, etc, all categories that determine how good a coffee is.&lt;/p&gt;
&lt;p&gt;
&lt;iframe src="https://crtut13.shinyapps.io/coffee_ratings/" width="100%" height="650" style="border-color: transparent;"&gt;
&lt;/iframe&gt;
&lt;br /&gt;
&lt;/p&gt;
&lt;p&gt;One of the fun things that I wanted to play around with was the &lt;code&gt;ggiraph&lt;/code&gt; package that allows you to create interactive elements in &lt;code&gt;ggplot&lt;/code&gt; figures. That’s how I got the tooltip that shows the ratings when you hover over different rating segments. You can find the code that I used to build the app &lt;a href="https://github.com/codyrtuttle/coffee_shiny_app"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are a few things that I wish were different - for one, there was no unique identifier or name for the coffees in the database, and there were lots of duplicate fields for different coffees, so I had to concatenate way too many fields (clearly) to get the identifier in the drop down filter. I’m considering going back and adding filters for all of those elements individually to make it a bit easier to handle.&lt;/p&gt;
&lt;p&gt;Anyways, just wanted to share the latest thing I’ve been working on. Check out the app and I’ll be back soon! The next post I’m thinking about is exploring the &lt;code&gt;tidymodels&lt;/code&gt; universe with a treasure trove of new COVID-19 data that the CDC just published. Stay tuned!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>61ab63a32ab6b3797ae6294048c661ed</distill:md5>
      <category>shiny</category>
      <category>R</category>
      <category>coffee</category>
      <guid>https://codyrtuttle.netlify.app/posts/2021-04-05-new-shiny-app-on-world-coffee-ratings</guid>
      <pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>Analyzing my 2020 running performance</title>
      <dc:creator>Cody Tuttle</dc:creator>
      <link>https://codyrtuttle.netlify.app/posts/2020-12-13-analyzing-my-running-performance-using-strava-data</link>
      <description>


&lt;p&gt;Hello, folks! In this post I’ll be looking at my running performance over the last year by analyzing my running data from the Strava app. Before I start that, here’s a bit of background on how I’ve experienced a new-found love of running over the last year and what led to this in the first place.&lt;/p&gt;
&lt;p&gt;I believe I’m not the first to point out that this year has been a weird one. Societally, it’s been mostly pretty shitty (until the second week in November when things started looking up a bit). The pandemic has upended every facet of life, hundreds of thousands of people have died needlessly, there were kiler hornets, and I’m sure I’m forgetting about ten other big terrible things that happened (feel free point them out if you feel so inclined). Personally, this year has been a mixed bag for my wife and me. We’ve felt all the terrible things going on in the world at large, and our lives look so much different than we thought they would. While we’ve had our fair share of pain and grief caused by the pandemic, we were left in a relatively good spot - we both work from home in stable jobs, we were able to buy a house in a neighborhood that we love, and we were able to plan to start our family. We don’t take that for granted at all, and we truly grieve with those whose lives have been completely devastated over the last year.&lt;/p&gt;
&lt;p&gt;When the pandemic shut everything down for us starting in mid March, one of the more noticeable things to stop for us was working out at the gym. In the wake of that, my wife and I decided to start running more. At the time we were living on the upper floor in a pretty small duplex so we didn’t have a whole lot of space to do anything more than very tame bodyweight workouts inside, and we knew that we needed a physical outlet to get away from the stress of the pandemic and of me finishing my final semester of grad school without the confines of our small living space.&lt;/p&gt;
&lt;p&gt;Enter running. I’ve never been a complete stranger to running over the course of my life, but I’ve also never really embraced it. There have been various fits and starts of running more frequently, but it’s never been what I would call sustained. Mar had periods of more dedicated, sustainable running over the last decade or so, even training for and running a half-marathon. She could always handle distance better than I could, but I liked running faster. Since we moved to Minneapolis in 2017 up until last March, Mar and I ran an average of once every week or two probably, never more than 2 or 3 miles at a time. We liked to run around our first neighborhood here, and there were a few months where we would go run a 3 ish mile loop around a nice lake every weekend. But I’ve never exactly loved running. I’ve done it because I know aerobic exercise is good for me and because it’s a good way for Mar and I to workout together every so often, but I’ve always liked lifting and bodyweights and sprint-type workouts a lot better. Until last March, the farthest I think I’d ever run was five miles tops, probably less. I could do a mile decently fast for a normal non-runner (I think I ran a sub-six minute mile a few times in college), but I never liked running longer distances than that.&lt;/p&gt;
&lt;p&gt;Fast forward to now, close to a year since we started that journey, I’ve run almost 1,000 miles since April, the majority of it coming in the last five or so months. Mar and I started running as a way of coping once things shutdown, but somewhere along the way it became a new thing for me, something that I genuinely started to love. We started out slow, running a mile and half or two most days, with a long run of 5.5 or 6 on Friday. Our first week of 20 miles, probably in late April or early May, was a big milestone, as was the time we ran 9 miles (thrice around our normal Friday 3 mile lake loop). June and July brought a bit of a slow-down again, with some minor injuries and tweaks, moving into our new house, and the heat of the summer. But in September I bought a new pair of running shoes (my first in probably six years) and started to get back into it again, and it was the start of a real thing for me. Mar and I still ran together from time to time, but she was in the throes of first trimester nausea, and I was increasing my mileage to 25 to 30 miles a week. I did a ten mile run in mid October at a decent pace, and that might have been what sealed it for me. In November I set out to do another 10-miler but through a weird series of events I ended up running my first half-marathon - my time was 1:50 ish, which while not competitive, is pretty good for someone who started running in earnest in March and had never run a half before. Since then I’ve been doing 30-35 mile weeks with some rest weeks here and there, and I’ve really gotten into it. I found my first running podcasts around a month ago, and started to plan my training more strategically based on what I was learning, easing my pace, incorporating strides, and dedicating one run every week or two to be more of an intervals workout.&lt;/p&gt;
&lt;p&gt;The throes of winter cold and snow have descended upon Minneapolis by now, so I imagine my training will slow down a bit until mid-March or so, but all of this to say that over the last 9 months I’ve found a new love of running that will be with me for a long time I think. Once the winter thaw comes I’d like to start training for my first marathon (even if it is just by myself, but hopefully I’ll be able to find a race by then), and who knows what will come after that.&lt;/p&gt;
&lt;p&gt;I’ve also started to combine this new-found love of running with my love of data. In October I started downloading my running data from Strava and it’s been fun to explore it since. Today’s post will be a more formal dive into my running performance over the last year, primarily to help me see how far I’ve come and allow me to be proud of myself (something I have a really hard time with most days). Here we go! ***&lt;/p&gt;
&lt;p&gt;So! To start. I’ll have two broad sections to this analysis. First I’ll do a basic descriptive look at my running performance in 2020, cutting the data by week, month, day of the week, etc. Second, I’ll make an interactive map of all my runs using &lt;code&gt;leaflet&lt;/code&gt; so we can visualize where I run and what my typical routes look like.&lt;/p&gt;
&lt;p&gt;I downloaded my Strava account data from the online site, which resulted in getting a big zipped folder of a lot of different files. I know I published this post in February of 2021, but I decided to just analyze the data from 2020 to keep it simple. There are two pieces of that I incorporate here. The first is an overview file of all my activities I’ve recorded on Strava, including the name, date, time, distance, pace, etc. The second is a folder of all of the GPS location data for all of the activities, in GPX format. I’ll get back to the GPX location files here in a bit, and start with the raw activities file.&lt;/p&gt;
&lt;p&gt;This code chunk loads my packages using &lt;code&gt;pacman&lt;/code&gt; and imports and cleans the data from the activities file. Most of the cleaning is converting paces and distances into the units I generally use (miles instead of kilometers, pace in minutes per mile, etc.). Also, shoutout to the &lt;code&gt;lubridate&lt;/code&gt; package for making the date-time parsing so painless.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;# install.packages(&amp;quot;leaflet&amp;quot;)

library(tidyverse)
library(readxl)
library(janitor)
library(lubridate)
library(gtools)
library(devtools)
library(leaflet)
library(gt)
library(sp)
library(maptools)

### load and clean activity summaries - no location data
strava &amp;lt;- read_excel(&amp;quot;strava_activities.xlsx&amp;quot;) %&amp;gt;% 
  clean_names() %&amp;gt;% 
  mutate(act_date = mdy_hms(activity_date), 
         act_date_time = (act_date - hours(5)),
         month = month(act_date_time, label = T), 
         day = day(act_date_time), 
         year = year(act_date_time), 
         mdy = format(act_date_time, format = &amp;quot;%m-%d-%Y&amp;quot;),
         week = floor_date(act_date_time, unit = &amp;quot;week&amp;quot;),
         wkday = wday(act_date_time, label = T),
         time = format(act_date_time, &amp;quot;%H:%M:%S&amp;quot;), 
         move_time = moving_time/60, 
         distance = distance_7/1.609, 
         avg_pace = move_time/distance, 
         avg_speed = average_speed/1.609, 
         max_speed = max_speed/1.609, 
         shoe = ifelse(activity_type == &amp;quot;Run&amp;quot; &amp;amp;
                         act_date_time &amp;gt;= as.Date(&amp;quot;2020-09-08&amp;quot;) &amp;amp; 
                         act_date_time &amp;lt;= as.Date(&amp;quot;2020-11-15&amp;quot;), &amp;quot;Ghosts&amp;quot;, 
                       ifelse(activity_type == &amp;quot;Run&amp;quot; &amp;amp; 
                                act_date_time &amp;lt;= as.Date(&amp;quot;2020-09-08&amp;quot;), &amp;quot;North Face faithfuls&amp;quot;, activity_gear)), 
         multiplier = distance/avg_pace, 
         performance = move_time*multiplier) %&amp;gt;% 
  select(act_date_time, month, day, year, mdy, week, wkday, 
         time, move_time, distance, avg_pace, avg_speed, 
         max_speed, multiplier, performance, shoe, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One thing I want to point out is the creation of the &lt;code&gt;multiplier&lt;/code&gt; and &lt;code&gt;performance&lt;/code&gt; variables towards the bottom of the chunk. I wanted a way to compare my runs in a more standardized way that didn’t just use distance or average pace as the metric. Those are instructive indicators, but only to a certain point; I can run long distances, but if my pace isn’t where I want it to be then it doesn’t mean as much. Likewise, I can run at a fast pace, but if I can’t sustain it for longer distances, it also doesn’t mean as much. The performance metric I thought of calculates the distance divided by pace as a multiplier for how long a run is. If I run 60ish minutes for 8 miles at 8 minutes a mile, I’ll get a multiplier of around 1, which leaves my performance at ~60. As opposed to if I run 60 minutes for 10 miles at 6 minutes a mile (which I absolutely can not do), the multiplier would be 1.4ish, and the performance would be 85ish. More on this later.&lt;/p&gt;
&lt;p&gt;First things first, I want to do a basic look at my running mileage over time. I’ll start by doing a look by month, then by week.&lt;/p&gt;
&lt;p&gt;Here’s the monthly look.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;strava %&amp;gt;% 
  filter(activity_type == &amp;quot;Run&amp;quot;) %&amp;gt;% 
  group_by(month) %&amp;gt;% 
  summarize(mileage = sum(distance, na.rm = T)) %&amp;gt;% 
  ggplot(aes(x = month, y = mileage)) +
  geom_col(fill = &amp;quot;#d98004&amp;quot;) +
  scale_y_continuous(limits = c(0,150), breaks = seq(0,150, by = 30)) +
  xlab(&amp;quot; &amp;quot;) +
  ylab(&amp;quot;Total Mileage&amp;quot;) +
  labs(title = &amp;quot;Cody&amp;#39;s Monthly Running Mileage \n&amp;quot;,
       caption = &amp;quot;Source: Strava&amp;quot;) +
  theme_classic() +
  theme(
    plot.title.position = &amp;quot;plot&amp;quot;, 
    plot.caption.position = &amp;quot;plot&amp;quot;,
    plot.title = element_text(size = 13)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3fd2536077bf_files/figure-html/unnamed-chunk-2-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;Now for weekly:&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;strava %&amp;gt;% 
  filter(activity_type == &amp;quot;Run&amp;quot;) %&amp;gt;% 
  group_by(week) %&amp;gt;% 
  summarize(mileage = sum(distance, na.rm = T)) %&amp;gt;% 
  ggplot(aes(x = week, y = mileage)) +
  geom_col(fill = &amp;quot;#d98004&amp;quot;) +
  scale_y_continuous(limits = c(0,50), breaks = seq(0,50, by = 10)) +
  xlab(&amp;quot; &amp;quot;) +
  ylab(&amp;quot;Total Mileage&amp;quot;) +
  labs(title = &amp;quot;Cody&amp;#39;s Weekly Running Mileage \n&amp;quot;,
       caption = &amp;quot;Source: Strava&amp;quot;) +
  theme_classic() +
  theme(
    plot.title.position = &amp;quot;plot&amp;quot;, 
    plot.caption.position = &amp;quot;plot&amp;quot;,
    plot.title = element_text(size = 13)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3fd2536077bf_files/figure-html/unnamed-chunk-3-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;These two graphs show more or less what I explained earlier. My mileage increased throughout spring and early summer 2020, plummeted during the mid-summer, and then started to pick back up again in the fall and through the end of the year to its highest levels. It’s interesting how much the weekly look reveals that the monthly hides - even in the valley of the summer and the peaks of the fall, there’s still a lot of ups and downs. Some of that variation is the ebb and flow of my work and recovery weeks, and some of it is the weather getting harder with snow and such. Though you can’t see it, this bouncy pattern has continued into the new year. One thing I’d like to work on being more consistent with my weekly mileage - trying to ease into smoother increases and decreases in mileage rather than the stark up and downs we see here.&lt;/p&gt;
&lt;p&gt;Obviously I knew approximately what these charts would look like. One thing I’m not as sure of how my runs break down on days of the week and time of day, both in terms of the number of runs and total mileage. This next graphs look at both the number of runs and total mileage by day of the week.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;strava %&amp;gt;% 
  group_by(wkday) %&amp;gt;% 
  count() %&amp;gt;% 
  ggplot(aes(x = wkday, y = n)) +
  geom_col(fill = &amp;quot;#0c0757&amp;quot;) +
  scale_y_continuous(limits = c(0,100), breaks = seq(0,100, by = 20)) +
  xlab(&amp;quot; &amp;quot;) +
  ylab(&amp;quot;Total Runs&amp;quot;) +
  labs(title = &amp;quot;Cody&amp;#39;s Top Running Days by Total Runs \n&amp;quot;,
       caption = &amp;quot;Source: Strava&amp;quot;) +
  theme_classic() +
  theme(
    plot.title.position = &amp;quot;plot&amp;quot;, 
    plot.caption.position = &amp;quot;plot&amp;quot;,
    plot.title = element_text(size = 13)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3fd2536077bf_files/figure-html/unnamed-chunk-4-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;strava %&amp;gt;% 
  group_by(wkday) %&amp;gt;% 
  summarize(mileage = sum(distance, na.rm = T)) %&amp;gt;% 
  ggplot(aes(x = wkday, y = mileage)) +
  geom_col(fill = &amp;quot;#0d0263&amp;quot;) +
  scale_y_continuous(limits = c(0,200), breaks = seq(0,200, by = 50)) +
  xlab(&amp;quot; &amp;quot;) +
  ylab(&amp;quot;Total Mileage&amp;quot;) +
  labs(title = &amp;quot;Cody&amp;#39;s Top Running Days by Mileage \n&amp;quot;,
       caption = &amp;quot;Source: Strava&amp;quot;) +
  theme_classic() +
  theme(
    plot.title.position = &amp;quot;plot&amp;quot;, 
    plot.caption.position = &amp;quot;plot&amp;quot;,
    plot.title = element_text(size = 13)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3fd2536077bf_files/figure-html/unnamed-chunk-4-2.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;This is new to me! Wednesdays were my biggest days for running overall and second biggest day for mileage - I wasn’t expecting that. I was thinking Fridays might be my biggest running day, but I guess not. Saturdays were my biggest days mileage-wise, which isn’t a surprise as that’s mroe a long run day for most runners (including me), but Wednesdays got close to it. The pattern seems to have been more runs with lower mileage on Wednesdays and fewer runs with higher mileage on Saturdays. So interesting.&lt;/p&gt;
&lt;p&gt;Now I want to go beyond the mileage piece and look more at the performance aspect that I touched on earlier. While it’s insightful to look at distance, it’s also instructive to think about my efficiency and speed as a runner. Enter the calculation I talked about above for performance. There are a few other caveats that I want to put in here though. While the calculation I came up with (imho) is pretty good and standardizes a lot of a run, it can fall victim to extremes, either mileage wise or speed wise. Middle distance and smooth pace is more reliable. So I’ll take it with a bit of a grain of salt, and you probably should, too.&lt;/p&gt;
&lt;p&gt;What I’ll do with the performance metric is chart it across month and week, probably as a median to do a better job of smoothing outliers during those periods. I had half a mind to do a simple regression to try to pick out the best predictors of my performance but honestly, the time series nature of it would make any significant or interesting results more or less useless. I don’t do time series analysis very much and it’s hard to do right so I figure I won’t wade into it this time. (Side note, I do want to get into a bit more modelling in this blog space, but I don’t think these are the right data. Maybe I could do that for my running when I have more months’ worth of data and it’s such a clearly linear trend of increased mileage and performance.)&lt;/p&gt;
&lt;p&gt;Here’s a look at my median performance by month.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;strava %&amp;gt;% 
  filter(activity_type == &amp;quot;Run&amp;quot;) %&amp;gt;% 
  group_by(month) %&amp;gt;% 
  summarize(performance = median(performance, na.rm = T)) %&amp;gt;% 
  ggplot(aes(x = month, y = performance)) +
  geom_col(fill = &amp;quot;#d98004&amp;quot;) +
  scale_y_continuous(limits = c(0,30), breaks = seq(0,30, by = 10)) +
  xlab(&amp;quot; &amp;quot;) +
  ylab(&amp;quot;Median Performance&amp;quot;) +
  labs(title = &amp;quot;Cody&amp;#39;s Monthly Running Performance \n&amp;quot;,
       caption = &amp;quot;Source: Strava&amp;quot;) +
  theme_classic() +
  theme(
    plot.title.position = &amp;quot;plot&amp;quot;, 
    plot.caption.position = &amp;quot;plot&amp;quot;,
    plot.title = element_text(size = 13)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3fd2536077bf_files/figure-html/unnamed-chunk-5-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;After seeing the grpahs for mileage, it shouldn’t be a surprise that my performance goes up continually once the fall settles in. Here’s the same look by week for a more granular look.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;strava %&amp;gt;% 
  filter(activity_type == &amp;quot;Run&amp;quot;) %&amp;gt;% 
  group_by(week) %&amp;gt;% 
  summarize(performance = median(performance, na.rm = T)) %&amp;gt;% 
  ggplot(aes(x = week, y = performance)) +
  geom_col(fill = &amp;quot;#d98004&amp;quot;) +
  scale_y_continuous(limits = c(0,45), breaks = seq(0,45, by = 15)) +
  xlab(&amp;quot; &amp;quot;) +
  ylab(&amp;quot;Median Performance&amp;quot;) +
  labs(title = &amp;quot;Cody&amp;#39;s Weekly Running Performance \n&amp;quot;,
       caption = &amp;quot;Source: Strava&amp;quot;) +
  theme_classic() +
  theme(
    plot.title.position = &amp;quot;plot&amp;quot;, 
    plot.caption.position = &amp;quot;plot&amp;quot;,
    plot.title = element_text(size = 13)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3fd2536077bf_files/figure-html/unnamed-chunk-6-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;This also checks out - not sure when the week is that I had the massive median performance. It could have been mid November when I ran my first half marathon. That would make sense.&lt;/p&gt;
&lt;p&gt;Now what I want to do is a map of my routes. I love maps and making them, so this is something I’m excited about it. There are a lot of things I could choose to do with this, like color the lines of my route by performance or by the date, but what I think I’ll do is keep things simple and just map the lines themselves. I run a fair few routes, but there are some that I do a lot more than others, so the map should make those lines thicker if I leave the color out of it.&lt;/p&gt;
&lt;p&gt;One thing I’ll say here is that a lot of the data cleaning for the next section is taken more or less directly from other people’s code on GitHub or StackOverflow. I wouldn’t have been able to do this if a lot of way smarter people than me hadn’t done what I’m doing and been kind enough to share their code online. I’ve never worked with GPX files before, but luckily I was able to adapt code someone wrote for a Strava R package. Fun fact, my computer didn’t seem to want to just download the package, so I was left to scour the source code and bring in what I needed to work with the folder of GPX files. Here’s the setup code, including the functions to parse the GPX data.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;### function to import and process GPX strava data files for locations

process_data &amp;lt;- function(path, old_gpx_format = FALSE) {
  # Function for processing a Strava gpx file
  process_gpx &amp;lt;- function(file) {
    # Parse GPX file and generate R structure representing XML tree
    pfile &amp;lt;- XML::htmlTreeParse(file = file,
                                error = function (...) {},
                                useInternalNodes = TRUE)
    
    coords &amp;lt;- XML::xpathSApply(pfile, path = &amp;quot;//trkpt&amp;quot;, XML::xmlAttrs)
    # extract the activity type from file name
    type &amp;lt;- stringr::str_match(file, &amp;quot;.*-(.*).gpx&amp;quot;)[[2]]
    # Check for empty file.
    if (length(coords) == 0) return(NULL)
    # dist_to_prev computation requires that there be at least two coordinates.
    if (ncol(coords) &amp;lt; 2) return(NULL)
    
    lat &amp;lt;- as.numeric(coords[&amp;quot;lat&amp;quot;, ])
    lon &amp;lt;- as.numeric(coords[&amp;quot;lon&amp;quot;, ])
    
    if (old_gpx_format == TRUE) {
      ele &amp;lt;- as.numeric(XML::xpathSApply(pfile, path = &amp;quot;//trkpt/ele&amp;quot;, XML::xmlValue))
    }
    
    time &amp;lt;- XML::xpathSApply(pfile, path = &amp;quot;//trkpt/time&amp;quot;, XML::xmlValue)
    
    # Put everything in a data frame
    if (old_gpx_format == TRUE) {
      result &amp;lt;- data.frame(lat = lat, lon = lon, ele = ele, time = time, type = type)
    } else {
      result &amp;lt;- data.frame(lat = lat, lon = lon, time = time, type = type)
    }
    result &amp;lt;- result %&amp;gt;%
      dplyr::mutate(dist_to_prev = c(0, sp::spDists(x = as.matrix(.[, c(&amp;quot;lon&amp;quot;, &amp;quot;lat&amp;quot;)]), longlat = TRUE, segments = TRUE)),
                    cumdist = cumsum(dist_to_prev),
                    time = as.POSIXct(.$time, tz = &amp;quot;GMT&amp;quot;, format = &amp;quot;%Y-%m-%dT%H:%M:%OS&amp;quot;)) %&amp;gt;%
      dplyr::mutate(time_diff_to_prev = as.numeric(difftime(time, dplyr::lag(time, default = .$time[1]))),
                    cumtime = cumsum(time_diff_to_prev))
    result
  }
  
  # Process all the files
  data &amp;lt;- gtools::mixedsort(list.files(path = path, pattern = &amp;quot;*.gpx&amp;quot;, full.names = TRUE)) %&amp;gt;%
    purrr::map_df(process_gpx, .id = &amp;quot;id&amp;quot;) 
}

# strava location data from GPX files
strava_locs &amp;lt;- process_data(&amp;quot;activities&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next tricky bit after just reading in the GPX files was figuring out how manipulate them into line segments rather than points, so I could map my routes. Luckily, Kyle Walker has a great &lt;a href="https://rpubs.com/walkerke/points_to_line"&gt;post&lt;/a&gt; on doing just that and created a handy function I was able to use, shown below.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;### function to clean points data to lines for easier mapping

points_to_line &amp;lt;- function(data, long, lat, id_field = NULL, sort_field = NULL) {
  
  # Convert to SpatialPointsDataFrame
  coordinates(data) &amp;lt;- c(long, lat)
  
  # If there is a sort field...
  if (!is.null(sort_field)) {
    if (!is.null(id_field)) {
      data &amp;lt;- data[order(data[[id_field]], data[[sort_field]]), ]
    } else {
      data &amp;lt;- data[order(data[[sort_field]]), ]
    }
  }
  
  # If there is only one path...
  if (is.null(id_field)) {
    
    lines &amp;lt;- SpatialLines(list(Lines(list(Line(data)), &amp;quot;id&amp;quot;)))
    
    return(lines)
    
    # Now, if we have multiple lines...
  } else if (!is.null(id_field)) {  
    
    # Split into a list by ID field
    paths &amp;lt;- sp::split(data, data[[id_field]])
    
    sp_lines &amp;lt;- SpatialLines(list(Lines(list(Line(paths[[1]])), &amp;quot;line1&amp;quot;)))
    
    # I like for loops, what can I say...
    for (p in 2:length(paths)) {
      id &amp;lt;- paste0(&amp;quot;line&amp;quot;, as.character(p))
      l &amp;lt;- SpatialLines(list(Lines(list(Line(paths[[p]])), id)))
      sp_lines &amp;lt;- spRbind(sp_lines, l)
    }
    
    return(sp_lines)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After that, I can convert my points data to lines, below.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;strava_locs_clean &amp;lt;- strava_locs %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  clean_names() %&amp;gt;% 
  rename(time_old = time) %&amp;gt;% 
  mutate(act_date = ymd_hms(as.character(time_old)), 
         act_date_time = (time_old - hours(5)),
         month = month(act_date_time), 
         day = day(act_date_time), 
         year = year(act_date_time), 
         mdy = format(act_date_time, format = &amp;quot;%m-%d-%Y&amp;quot;),
         week = week(act_date_time),
         wkday = wday(act_date_time, label = T),
         time = format(act_date_time, &amp;quot;%H:%M:%S&amp;quot;))

strava_lines &amp;lt;- points_to_line(
  data = strava_locs_clean, 
  long = &amp;quot;lon&amp;quot;, 
  lat = &amp;quot;lat&amp;quot;, 
  id_field = &amp;quot;id&amp;quot;, 
  sort_field = &amp;quot;cumtime&amp;quot;
)

line_match_run &amp;lt;- strava_locs_clean %&amp;gt;% 
  filter(cumtime == 0) %&amp;gt;% 
  left_join(strava, by = &amp;quot;act_date_time&amp;quot;) %&amp;gt;% 
  arrange(id)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I need to join the spatial lines table (&lt;code&gt;strava_lines&lt;/code&gt;) back in with the rest of the fields for the activities to create a spatial lines data frame. This is mainly so I can filter down to just runs and get the hikes and walks out of there. (I also have a few activities that are labelled as runs that are actually when I was playing tennis, so I’ll get those out of there too.) The reason I do subset instead of &lt;code&gt;dplyr&lt;/code&gt;’s filter is because the spatial lines data frames don’t work well with tidy verse. I suppose I could have filtered the rows before joining and just done an inner join, but whatever.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;strava_sldf &amp;lt;- SpatialLinesDataFrame(strava_lines, line_match_run, match.ID = F)

strava_run_sldf &amp;lt;- subset(strava_sldf, strava_sldf$activity_type == &amp;quot;Run&amp;quot; &amp;amp; 
                            !grepl(&amp;quot;tennis&amp;quot;, strava_sldf$activity_name, ignore.case = T))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’re ready to map! My go-to package for mapping is &lt;code&gt;leaflet&lt;/code&gt; - so simple and intuitive to build and there are some great features. I won’t build too complicated of a map here, but I’m sure I’ll make more advanced maps in future posts. Here I’m choosing to zoom my map to northeast Minneapolis because that’s where the majority of my runs originate. However, I have run in a few other places and recorded on Strava, like a few trails in some Twin Cities suburbs, and a few in Grand Marais, on the north shore of Lake Superior.&lt;/p&gt;
&lt;p&gt;The great thing about &lt;code&gt;leaflet&lt;/code&gt; maps is that they are interactive, though! So you can pan to different parts of the city to follow routes, and you can zoom in and out to look at routes more closely. You should also zoom out a bit and see if you can find those other routes that aren’t in Minneapolis, too!&lt;/p&gt;
&lt;p&gt;The lines are darker when there are more routes there, and lighter when there are fewer routes. Zooming in really close to the lines will show you just how many routes are there.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;map &amp;lt;- leaflet::leaflet(data = strava_run_sldf) %&amp;gt;% 
  addProviderTiles(providers$CartoDB.Positron) %&amp;gt;%
  addPolylines(weight = 3) %&amp;gt;% 
  setView(-93.2474, 45.0132, zoom = 12.5)

htmlwidgets::saveWidget(map, &amp;quot;strava_map.html&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;
&lt;iframe src="strava_map.html" width="100%" height="650" style="border-color: transparent;"&gt;
&lt;/iframe&gt;
&lt;br /&gt;
&lt;/p&gt;
&lt;p&gt;But this is all for now. I could do a lot more with this, but honestly, I just want to publish this. I’ve been working on it for several months now and I’ve only just worked up the motivation to finish it. So here it is! Woo!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>1e21a32ecfaa8082361746812a4cc2fb</distill:md5>
      <category>R</category>
      <category>running</category>
      <category>strava</category>
      <category>leaflet</category>
      <guid>https://codyrtuttle.netlify.app/posts/2020-12-13-analyzing-my-running-performance-using-strava-data</guid>
      <pubDate>Sat, 27 Mar 2021 00:00:00 +0000</pubDate>
      <media:content url="https://codyrtuttle.netlify.app/posts/2020-12-13-analyzing-my-running-performance-using-strava-data/analyzing-my-running-performance-using-strava-data_files/figure-html5/unnamed-chunk-2-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>A true welcome</title>
      <dc:creator>Cody Tuttle</dc:creator>
      <link>https://codyrtuttle.netlify.app/posts/2020-12-07-a-true-welcome</link>
      <description>Why I'm here</description>
      <category>no code</category>
      <category>intro</category>
      <guid>https://codyrtuttle.netlify.app/posts/2020-12-07-a-true-welcome</guid>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
    </item>
    <item>
      <title>CFPB Analysis</title>
      <dc:creator>Cody Tuttle</dc:creator>
      <link>https://codyrtuttle.netlify.app/posts/2020-12-02-cfpb-analysis</link>
      <description>


&lt;p&gt;*Update: repurposing this post for my new blog site! will actually have a new post detailing catching everyone up on everything soon&lt;/p&gt;
&lt;p&gt;Hello everyone! Welcome back to my blog - it’s been way too long, I know, but I’m back for a post before I start my new semester (my final semester of grad school HALLELUJAH). As much as I want to update you on my life and everything, I’m going to skip the pleasantries and just get straight to the post. I don’t really have time to get into all that I want to with the data as it is, so I probably shouldn’t take the time to update you on my life when you probably wouldn’t read it anyways.&lt;/p&gt;
&lt;p&gt;In this post I’m going to be looking at data from the &lt;a href="https://www.consumerfinance.gov/data-research/financial-well-being-survey-data/"&gt;2016 National Financial Well-Being Survey&lt;/a&gt;, published by the Consumer Financial Protection Bureau. As you can probaby guess, it’s a survey about people’s financial well-being, with questions about financial skills, behaviors, attitudes, experiences, etc., as well as demographics like race, employment, poverty status, urban/rural, etc. Since interning at the Federal Reserve Bank last summer, I’ve definitely gained more of an interest in financial issues and credit and such, especially racial inequities within access to credit and financial services. So that’s what I’ll be looking at in this post. There’s a lot in these data that I won’t have time or space to go into wtih this post that I’d like to - urban/rural differences, gender differences, educational differences, regional differences, not to mention all of the financial measures that I won’t get to look at.&lt;/p&gt;
&lt;p&gt;(As a quick plug, I found the data from a site called &lt;a href="https://tinyletter.com/data-is-plural"&gt;Data is Plural&lt;/a&gt;, which has a spreadsheet of hundreds of interesting data sets, updated every week. Check it out.)&lt;/p&gt;
&lt;p&gt;The main thing that I want to look at in this post is racial differences in access to credit, and whether/how that can be explained by poverty status and financial well-being and financial skill. One of the central features of banking and lending history in America is racial discrimination. Practices such as &lt;a href="https://www.citylab.com/equity/2018/04/how-the-fair-housing-act-failed-black-homeowners/557576/"&gt;redlining&lt;/a&gt; shaped the current urban housing landscape, and though now illegal, discriminatory lending practices are still rampant. Several years ago there was a decently prominant &lt;a href="https://www.revealnews.org/article/for-people-of-color-banks-are-shutting-the-door-to-homeownership/"&gt;story by Reveal News&lt;/a&gt; discussing how people of color and minorities are more likely to be turned down for a loan than whites across many metro areas. Defenders of this dynamic say that its justified by lower credit scores for minorities, and insist that black people are just worse with money and aren’t as credit worthy. While credit scores aren’t public data and therefore can’t be explored as factor in racial disparities in lending, &lt;a href="suffolklawreview.org/wp-content/uploads/2014/01/Rice-Swesnik_Lead.pdf"&gt;studies&lt;/a&gt; have shown that credit score algorithms have a discriminatory impact on people of color. So even if credit scores do account for differences in lending, that doesn’t explain away discrimination.&lt;/p&gt;
&lt;p&gt;With that, let’s get into the data.&lt;/p&gt;
&lt;h3 id="data-loading-and-survey-weights"&gt;Data Loading and Survey Weights&lt;/h3&gt;
&lt;p&gt;Here I simply load in my data (already mostly cleaned thanks to a handy R script provided by CFPB) and set it up as a survey. I decided to use the &lt;code&gt;srvyr&lt;/code&gt; package, which lets you work with survey dataframes in tidyverse style. After adding the survey weights in the code below, we should be good to go.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;data &amp;lt;- read.csv(&amp;quot;/Users/codytuttle/Desktop/cfpb_blog/cfpb_clean.csv&amp;quot;)
library(tidyverse)
library(survey)
library(srvyr)
weighted &amp;lt;- data %&amp;gt;%
  as_survey(PUF_ID, weight = finalwt) %&amp;gt;%
  rename(race = PPETHM, 
         incomecat = PPINCIMP) %&amp;gt;%
  mutate(rejected = ifelse(REJECTED_1 == &amp;quot;Yes&amp;quot;, 1, 
                           ifelse(REJECTED_1 == &amp;quot;No&amp;quot;, 0, 99)))&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="access-to-credit"&gt;Access to Credit&lt;/h3&gt;
&lt;p&gt;The first thing that I want to do is look at overall racial disparities in access to credit using a survey question that asks whether a respondent applied for credit and was turned down in the last year.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;weighted %&amp;gt;%
  group_by(race) %&amp;gt;%
  filter(rejected != 99) %&amp;gt;%
  summarize(rejected = survey_mean(rejected)) %&amp;gt;%
  ggplot(aes(x = race, y = 100*rejected, group = race)) +
    geom_bar(stat = &amp;quot;identity&amp;quot;, fill = &amp;quot;blue&amp;quot;) +
    theme_minimal() +
    labs(x = &amp;quot;Racial Group&amp;quot;, y = &amp;quot;% rejected&amp;quot;) +
    ggtitle(&amp;quot;Credit Rejection by Racial Group, 2016&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3fd220619cb7_files/figure-html/unnamed-chunk-2-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;This chart shows astounding racial disparities in lending - black respondents were over twice as likely to report being turned down for a loan in the past year than whites, and Hispanics were nearly 50% more likely than whites.&lt;/p&gt;
&lt;p&gt;However, while these differences are stark, in and of themselves they don’t necessarily constitute discrimination. There are certain characteristics not accounted for in that graph, primarily income, that could account for the observed differences. To take one step further in exploring whether or not these disparities could be evidence of discrimination, below I chart the same measure of being rejected for a loan by both income category and race. I limit the racial categories to just black and white, both to focus on the starkest disparities and to minimize clutter on the graph.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;weighted %&amp;gt;%
  filter(race == &amp;quot;Black, Non-Hispanic&amp;quot; | race == &amp;quot;White, Non-Hispanic&amp;quot;, rejected != 99) %&amp;gt;%
  group_by(race, incomecat) %&amp;gt;%
  summarize(rejected = survey_mean(rejected)) %&amp;gt;%
  ggplot(aes(x = incomecat, y = 100*rejected, group = race, fill = race)) +
    geom_bar(stat = &amp;quot;identity&amp;quot;, position = &amp;quot;dodge&amp;quot;) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1)) +
    labs(x = &amp;quot;Income Category&amp;quot;, y = &amp;quot;% rejected&amp;quot;, fill = &amp;quot;Racial Group&amp;quot;) +
    ggtitle(&amp;quot;Credit Rejection by Income Category and Racial Group, 2016&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3fd220619cb7_files/figure-html/unnamed-chunk-3-1.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;Here we can see that the racial disparities in lending observed in the first chart persist even after controlling for income, and may even be starker. Across every single income category, black respondents are rejected at a higher rate than white respondents, generally at a &lt;em&gt;two to three times higher rate&lt;/em&gt;, at least. If the decision to lend is about the borrower’s ability to pay it back, then income should be a pretty good metric. But even holding income equal, there are still massive racial disparities in lending.&lt;/p&gt;
&lt;p&gt;However, givng them the benefit of the doubt, maybe lenders are picking up on some other factor besides race that are causing them to reject disproportionately more black folks. In the next graph I look at the financial skills score by race and by income category and race. Even if it’s not a measure that’s directly used by lenders, it still probably can be a proxy for overall creditworthiness.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;weighted %&amp;gt;%
  filter(race == &amp;quot;Black, Non-Hispanic&amp;quot; | race == &amp;quot;White, Non-Hispanic&amp;quot;) %&amp;gt;%
  group_by(race) %&amp;gt;%
  summarize(finskill = survey_mean(as.numeric(FSscore))) %&amp;gt;%
  ggplot(aes(x = race, y = finskill)) +
    geom_bar(stat = &amp;quot;identity&amp;quot;, fill = &amp;quot;blue&amp;quot;) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1)) +
    labs(x = &amp;quot;Race&amp;quot;, y = &amp;quot;Financial Skills Score&amp;quot;) +
    ggtitle(&amp;quot;Financial Skills Score by Race, 2016&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3fd220619cb7_files/figure-html/unnamed-chunk-4-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;weighted %&amp;gt;%
  filter(race == &amp;quot;Black, Non-Hispanic&amp;quot; | race == &amp;quot;White, Non-Hispanic&amp;quot;) %&amp;gt;%
  group_by(race, incomecat) %&amp;gt;%
  summarize(finskill = survey_mean(as.numeric(FSscore))) %&amp;gt;%
  ggplot(aes(x = incomecat, y = finskill, group = race, fill = race)) +
    geom_bar(stat = &amp;quot;identity&amp;quot;, position = &amp;quot;dodge&amp;quot;) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1)) +
    labs(x = &amp;quot;Income Category&amp;quot;, y = &amp;quot;Financial Skills Score&amp;quot;, fill = &amp;quot;Racial Group&amp;quot;) +
    ggtitle(&amp;quot;Financial Skills Score by Income Category and Race, 2016&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3fd220619cb7_files/figure-html/unnamed-chunk-4-2.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;Well, it doesn’t look like giving lenders the benefit of the doubt is working out very well - both overall and across each income category, black respondents actually have very similar financial skills scores as whites, if not higher.&lt;/p&gt;
&lt;p&gt;Bear with me, because now I’m about to get a bit more weedsy. In the next few graphs I’m going to really try and get to the bottom of this issue as best as I can using these data and without using any fancy statistical techniques (this is neither the time nor the place for that). These charts will look at the financial skills scores of the following groups: all rejected applicants; all accepted applicants; rejected white applicants vs. accepted black applicants; and rejected black applicants vs. accepted white applicants.&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;weighted %&amp;gt;%
  filter(race == &amp;quot;Black, Non-Hispanic&amp;quot; | race == &amp;quot;White, Non-Hispanic&amp;quot;, rejected == 0) %&amp;gt;%
  group_by(race, incomecat) %&amp;gt;%
  summarize(finskill = survey_mean(as.numeric(FSscore))) %&amp;gt;%
  ggplot(aes(x = incomecat, y = finskill, group = race, fill = race)) +
    geom_bar(stat = &amp;quot;identity&amp;quot;, position = &amp;quot;dodge&amp;quot;) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1)) +
    labs(x = &amp;quot;Income Category&amp;quot;, y = &amp;quot;Financial Skills Score&amp;quot;, fill = &amp;quot;Racial Group&amp;quot;) +
    ggtitle(&amp;quot;Financial Skills Score of Rejected Credit Applicants by Income and Race, 2016&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3fd220619cb7_files/figure-html/unnamed-chunk-5-1.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;weighted %&amp;gt;%
  filter(race == &amp;quot;Black, Non-Hispanic&amp;quot; | race == &amp;quot;White, Non-Hispanic&amp;quot;, rejected == 1) %&amp;gt;%
  group_by(race, incomecat) %&amp;gt;%
  summarize(finskill = survey_mean(as.numeric(FSscore))) %&amp;gt;%
  ggplot(aes(x = incomecat, y = finskill, group = race, fill = race)) +
    geom_bar(stat = &amp;quot;identity&amp;quot;, position = &amp;quot;dodge&amp;quot;) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1)) +
    labs(x = &amp;quot;Income Category&amp;quot;, y = &amp;quot;Financial Skills Score&amp;quot;, fill = &amp;quot;Racial Group&amp;quot;) +
    ggtitle(&amp;quot;Financial Skills Score of Accepted Credit Applicants by Income and Race, 2016&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3fd220619cb7_files/figure-html/unnamed-chunk-5-2.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;weighted %&amp;gt;%
  filter((race == &amp;quot;Black, Non-Hispanic&amp;quot; &amp;amp; rejected == 0) | 
           (race == &amp;quot;White, Non-Hispanic&amp;quot; &amp;amp; rejected == 1)) %&amp;gt;%
  group_by(race, incomecat) %&amp;gt;%
  summarize(finskill = survey_mean(as.numeric(FSscore))) %&amp;gt;%
  ggplot(aes(x = incomecat, y = finskill, group = race, fill = race)) +
    geom_bar(stat = &amp;quot;identity&amp;quot;, position = &amp;quot;dodge&amp;quot;) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1), 
          plot.title = element_text(size = 10)) +
    labs(x = &amp;quot;Income Category&amp;quot;, y = &amp;quot;Financial Skills Score&amp;quot;, fill = &amp;quot;Racial Group&amp;quot;) +
    ggtitle(&amp;quot;Financial Skills Score of Rejected White Credit Applicants vs. 
            Accepted Black Applicants by Income, 2016&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3fd220619cb7_files/figure-html/unnamed-chunk-5-3.png" width="672" /&gt;&lt;/p&gt;
&lt;pre class="r"&gt;&lt;code&gt;weighted %&amp;gt;%
  filter((race == &amp;quot;Black, Non-Hispanic&amp;quot; &amp;amp; rejected == 1) | 
           (race == &amp;quot;White, Non-Hispanic&amp;quot; &amp;amp; rejected == 0)) %&amp;gt;%
  group_by(race, incomecat) %&amp;gt;%
  summarize(finskill = survey_mean(as.numeric(FSscore))) %&amp;gt;%
  ggplot(aes(x = incomecat, y = finskill, group = race, fill = race)) +
    geom_bar(stat = &amp;quot;identity&amp;quot;, position = &amp;quot;dodge&amp;quot;) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1), 
          plot.title = element_text(size = 10)) +
    labs(x = &amp;quot;Income Category&amp;quot;, y = &amp;quot;Financial Skills Score&amp;quot;, fill = &amp;quot;Racial Group&amp;quot;) +
    ggtitle(&amp;quot;Financial Skills Score of Rejected Black Credit Applicants vs. 
            Accepted White Applicants by Income, 2016&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src="file3fd220619cb7_files/figure-html/unnamed-chunk-5-4.png" width="672" /&gt;&lt;/p&gt;
&lt;p&gt;Here we go: First, for all rejected credit applicants, we see the same pattern of black respondents having similar or slightly higher financial skills scores than white respondents. Second, the pattern holds for all accepted applicants. No surprises there.&lt;/p&gt;
&lt;p&gt;Third, when we look at rejected black applicants vs. accepted white applicants, in order for there to be any justification of disparities at all, rejected black repondents should have clearly lower financial skills scores than accpeted white applicants. We do see this across several income categories, but it doesn’t at all appear to be the norm - we see very similar financial skills between rejected blacks and accepted whites for three categories, and rejected blacks even have moderately &lt;em&gt;higher&lt;/em&gt; financial skills scores than accepted whites for one income category.&lt;/p&gt;
&lt;p&gt;Fourth, in order to justify the somewhat similar or even higher financial skills scores for rejected black applicants vs. accepted whites, we should see a similar pattern for rejected whites vs. accepted blacks (i.e., sometimes rejected whites have similar or higher financial skills than accepted blacks). However, we don’t see this pattern at all - across every income bracket, accepted black applicants have far and away higher financial skills scores than rejected whites.&lt;/p&gt;
&lt;p&gt;In other words, the decision to reject a white applicant vs. accepting a black applicant with a similar income is clearly justified, at least in terms of financial skills, while the same cannot necessarily be said for rejecting a black applicant. To me, this suggests that black applicants for credit are held to a much higher standard than white appliants with similar credentials. In order for the observed disparities in credit rejection to be justified, there would have to be very clear differences between the many black applicants that are rejected and the whites that are accepted. However, we don’t see that - the disparities persist even after controlling for income, and often times financial skills as well.&lt;/p&gt;
&lt;p&gt;This to me is evidence that the observed racial disparities in lending are indeed indicative of discrimination. At the very least, even if not outright discrimination, it’s clear that black applicants are held to a much higher standard in credit decisions than whites with similar incomes. It would be foolish of me to claim that this analysis represents any kind of causal or definitive proof of discrimination - this is just one measure based on a survey that doesn’t account for actual credit score. But in my view, it represents one more piece of evidence that across so many sectors, banking and credit included, black people and people of color are not treated the same as whites.&lt;/p&gt;
&lt;p&gt;As always, I welcome your questions, comments, and feedback. I doubt I’ll be able to post again until this summer, but I’m really glad I was able to do this before my semester started again. Until next time!&lt;/p&gt;
&lt;pre class="r distill-force-highlighting-css"&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
      <distill:md5>fc4fb9865b7d4378235c5d76095054fd</distill:md5>
      <category>R</category>
      <category>CFBP</category>
      <category>equity</category>
      <category>banking</category>
      <guid>https://codyrtuttle.netlify.app/posts/2020-12-02-cfpb-analysis</guid>
      <pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate>
      <media:content url="https://codyrtuttle.netlify.app/posts/2020-12-02-cfpb-analysis/cfpb-analysis_files/figure-html5/unnamed-chunk-2-1.png" medium="image" type="image/png" width="1248" height="768"/>
    </item>
    <item>
      <title>Welcome to CRT Blog</title>
      <dc:creator>Cody Tuttle</dc:creator>
      <link>https://codyrtuttle.netlify.app/posts/welcome</link>
      <description>Hi, I'm Cody! Welcome to my new blog. I hope you enjoy 
reading what we have to say!</description>
      <guid>https://codyrtuttle.netlify.app/posts/welcome</guid>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
    </item>
  </channel>
</rss>
