[
  {
    "path": "posts/2022-09-13-thoughts-on-poverty/",
    "title": "Thoughts on poverty",
    "description": "Raw and unfiltered thoughts from the new Census poverty numbers.",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2022-09-13",
    "categories": [
      "no code",
      "poverty"
    ],
    "contents": "\nHey folks - I don’t have much time to post right now, but in the\nspirit of posting more frequently and being less of a perfectionist, I’m\ngoing to write for 15 minutes about my current musings and thoughts on\npoverty. The new poverty numbers from the Census Bureau came out today,\nand they’ve got me somewhere in between my head and my heart reflecting\non the dynamics of poverty - the intellectual and scholarly\nunderstanding, the political economy and societal optics, the lived\nreality and experience of people in poverty. With all of this, I’m just\ngoing to take 15 minutes and just get my thoughts down in writing. Like\nI alluded to, this is probably going to be something in between\nintellectual/academic musings and personal/emotional reflection. But\nwhatever it ends up actually being, it’s the honest and raw thoughts\nthat are swirling in my brain right now. I hope to find the time soon to\ndo a little more digging into these reflections, both from a conceptual\nand philosophical perspective, but also the data and research.\nHere it goes:\nI will start out with an obvious but I think important qualification\non all of these thoughts: I am a very privileged white male living in an\nupper-middle class neighborhood with a poverty rate of next to nothing.\nI have never experienced real poverty aside from being a broke college\nstudent. I didn’t experience poverty growing up as a kid, nor have I\nexperienced poverty as an adult. I live in a decently large city, so I\nsee poverty around me fairly frequently, but I am not residentially- or\nsocially-proximal to poverty in the way that I’m reflecting on now.\nThe new poverty numbers have me considering how complex and dynamic\npoverty truly is. At a very top line, without actually reading into the\ndetails just yet, the Census Bureau released the poverty statistics for\n2021, and poverty by most conceptions is down considerably from 2020,\nespecially child poverty. This is a true dynamic, and it’s a good thing,\nfull stop. But the official poverty numbers, whether in the form of the\nofficial poverty measure or the supplemental poverty measure, do not\neven come close to really capturing the experience of people living in\nfinancial and material deprivation.\nOfficial statistics, whether absolute (like the official poverty\nmeasure) or relative (like the supplemental poverty measure) are based\non hard thresholds and cutoffs for who falls into the “poverty” category\nand who does not, because they have to be. But what these cutoffs miss\nis the experience of people who are just beyond that bright line -\npeople who have income a little higher than whatever threshold is being\nused, but are actually no better off than the folks just below it. So\nmuch policy, so many benefits, so much scholarship and research, is\nbased on these arbitrary lines, and it leaves out the real experience of\nso many people. So many families are denied benefits because they have\nincome a few dollars higher than the poverty criteria being used,\nbenefits that the new numbers and years of evidence show actually help\nfamilies improve their circumstances and life prospects.\nAnother thing is that these numbers, especially the official poverty\nmeasure, are based on calculations taken at one point during the year.\nThey’re a sampling of folks who were above and below the cutoff at one\npoint during the year. But this completely missed the inherent dynamic\nnature of poverty. Lots of research shows that it’s just really really\neasy to fall into poverty. Most families and individuals are just one\nemergency or accident away from slipping into devastating financial\ncircumstances that could completely redefine their entire life\ntrajectory. My family is just one emergency\naway from this.\nAnd yet. Something that the new numbers also miss is that communities\nof color and other historically and currently marginalized groups are\nso much more likely to experience poverty than white folks. The\ntruth is that my family and I are most likely able to weather an\nemergency or shock a lot more easily than another family that looks just\nlike us but are Black, simply because of the societal and generational\nprivilege we are privy to.\nPoverty is complex and dynamic. There are so many ways of knowing\nthat we need to tap into - not just the official (or supplemental)\nstatistics, but deeper quantitative analysis that looks at distributions\nand timing, and most importantly, real qualitative lived experience of\nfolks in and near poverty.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-09-13T21:07:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-10-nyt-bestsellers/",
    "title": "NYT bestsellers",
    "description": "An exploration of NYT bestselling authors using {reactable}.",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2022-08-10",
    "categories": [
      "R",
      "reactable",
      "reading",
      "books",
      "NYT"
    ],
    "contents": "\nHey folks! I’m back again, so soon after my last post. Like I\nmentioned in my last post, I’m going to start posting more, shorter,\nslightly less-polished articles, and this is the start.\nToday I’m looking at New York Times bestselling authors. Tidy Tuesday\nposted\nthese data several months ago, and I was interested in exploring it\na bit. Around the same time I started seeing a lot more folks using the\n{reactable} package for making interactive tables in R. So I figured it\nmight be a good opportunity to test it out on the NYT data.\nOut of the spirit of shorter, less-polished posting, along with the\nfact that I’m definitely not a {reactable} expert, I’m not going to be\nmake this a step-by-step tutorial on how to use the package and make the\ntable. Rather, I’ll just drop all the code and you can see the finished\nproduct.\nFirst, the preliminaries of loading packages, data, and requisite\ndata cleaning.\n\n\n# load packages\nlibrary(tidyverse)\nlibrary(reactable)\n\n# load data\nnyt_titles <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-10/nyt_titles.tsv')\n\nnyt_full <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-10/nyt_full.tsv')\n\n\n\n\n# initial data cleaning\ntop_authors <- nyt_full %>% \n  filter(rank == 1) %>% \n  count(author) %>% \n  arrange(desc(n))\n\nbooks_at_one <- nyt_full %>% \n  filter(rank == 1) %>% \n  group_by(author, title) %>% \n  summarise(\n    weeks_at_one = n(), \n    week_start = min(week), \n    week_end = max(week)\n  ) %>% \n  arrange(desc(weeks_at_one))\n\n\nNow, the {reactable}!\n\n\nreactable(\n  top_authors,\n  filterable = T,\n  columns = list(\n    author = colDef(name = \"Author\"), \n    n = colDef(name = \"Weeks at #1\")\n  ), \n  details = function(index) {\n    sec_lvl = books_at_one[books_at_one$author == top_authors$author[index], ] \n    reactable(\n      sec_lvl, \n      columns = list(\n        author = colDef(name = \"Author\"), \n        title = colDef(name = \"Book\"), \n        weeks_at_one = colDef(name = \"Weeks at #1\"), \n        week_start = colDef(name = \"Start Week\"), \n        week_end = colDef(name = \"End Week\")\n      )\n    )\n  }\n)\n\n\n\nAnd voila! An interactive, drill-down table of the authors with the\nmost weeks atop the NYT bestseller list, complete with the ability to\nsearch authors and view which books were at number one for them.\nThis is it for now. I’ll be back soon with another short post!\nCheers, everyone!\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-10T22:20:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-06-quick-updates-and-musings/",
    "title": "Quick updates and musings",
    "description": "A brief note about where I've been and where I might be going.",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2022-08-06",
    "categories": [
      "no code"
    ],
    "contents": "\nI know it’s been ten thousand years since I posted (ok not really\nthat long, but still pretty long), but I wanted to get back on here and\nrenew my intentions of actually posting on here semi-regularly. The last\nyear or so has been decently bonkers with my daughter, but I really do\nwant to make time for this. Blogging and data exploration is something\nthat really has brought me a lot of joy.\nI guess the reason that I’m writing this now (and hopefully pushing\nit live before Liv wakes up from her nap) is because I’ve been seeing SO\nMUCH about the new Quarto platform from RStudio (soon to be Posit?). I\nwent down a rabbit hole for a few hours this week of trying to figure\nout converting my website from distill to the new Quarto system or\ncreating a new separate “musings” style blog using Quarto. But at the\nend of the day I realized that there is plenty of time for that, and I\ndon’t need to rush into it now - the more helpful and productive\npractice is actually just making use of the blog I have now.\nSo here we are, getting back into it.\nThis is my first new post (albeit not a real post), and from here on\nout, I’d like to mainly do small pieces that don’t necessarily have to\nbe super well-thought out or composed. The perfectionism is what keeps\nme from posting so much of the time, and I want to start to free myself\nfrom that. Occasionally I’m sure I’ll do longer deep dives and\nwalk-throughs, but for now I just want to continue to take the chance to\nwrite, to explore data that interests me without the self-expectations\nof it being perfectly insightful. Consistency and progress are my\ngoals.\nI’m committing here now that the next post I do will be soon, and it\nwill be just a small interactive table showing NYT best sellers.\nTidyTuesday released a data set on NYT best seller lists, and I played\naround with it a few months ago. I already have the code and and the\ntable basically ready, so now I just need to put it in post form and\npublish it.\nBut I wanted to do this post first, as a way to re-commit myself to\nthis. We’ll see where it takes me, and I sincerely appreciate the\nat-most five people who will read this and follow along.\nCheers until next time!\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-10T20:51:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-07-analysis-of-major-marathon-winners/",
    "title": "Analysis of Major Marathon Winners",
    "description": "A brief foray into winners of world major marathons, 1970-2018.",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2022-02-07",
    "categories": [
      "R",
      "running",
      "marathons"
    ],
    "contents": "\nWell folks, it’s hard to believe that it’s been 5 months since I’ve\nlast posted, and close to a year since I’ve posted anything with any\ntype of actual data analysis. C’est la vie with an infant in a\npandemic.\nI have been itching to get back to posting more semi-regularly\nthough, and to take steps towards making that happen, I decided to just\nstart small and get something out there. In that vein, today’s post will\nbe an analysis of the winners of world major marathons from the 1970s\nuntil 2018. I found a small data set on data.world of the names,\nnationalities, and times of the winners of the world’s biggest\nmarathons, and instead of being disappointed in the lack of data (age,\nsplits, more runners, what have you), I decided to take it as a chance\nto do a simple analysis on what was there.\n\n\n\n\n\n\nThere are six annual marathons that are considered “world major\nmarathons” and those are in Berlin, Boston, Chicago, London, New York\nCity, and Tokyo. Hundreds of elite runners from all over the world\ncompete to win these races every year, in addition to the thousands of\nrecreational runners looking to break personal bests and push their\nlimits. The data I’m working with today only contain the winners of\nthese marathons (both male and female), from 1970ish to 2018 (though\nthere’s data going back to 1897 for the Boston marathon. Like I said,\nthe data are pretty sparse - they only have name, nationality, gender,\nand time (in addition to the year and marathon won).\nI’ll start off by looking at some simple descriptive angles of the\ndata - how many individual runners and nationalities are represented in\nthe data, and how does it differ by marathon and gender.\n\n\n\nOverall, there are 316 distinct runners and 38 distinct nationalities\nrepresented in the 514 race results in the data. Honestly that’s\nconsiderably more nationalities than I thought would be represented. My\nprior was that East Africans (Kenya, Ethiopia) would make up the lion’s\nshare, with maybe 10 or 15 other countries popping up here and there -\nshows how much I know.\nNow I’ll do this cut by gender and marathon, in the tables below\n(shout out to the {gt} package!). Here’s how it looks by\ngender:\n\n\nGender\n      Distinct Races\n      Distinct Runners\n      Distinct Nationalities\n      Runners %\n      Nats %\n    Female\n233\n119\n25\n51.07%\n10.73%Male\n303\n196\n32\n64.69%\n10.56%\n\nFrom this table, it looks like there are both more distinct winners\nand more nationalities represented among winning men than women. As a\npercentage of races represented, there are more distinct winners among\nmen than women - there’s a distinct winner in ~64% of races for men, and\nonly a distinct winner in ~51% of races for women. For nationalities,\nthough it looks like there are equal ratios of distinct nationalities to\nraces for men and women (distinct nationalities in ~10% of races).\nNow let’s look at the same table, only cut by marathon instead of\ngender.\n\n\nMarathon\n      Distinct Races\n      Distinct Runners\n      Distinct Nationalities\n      Runners %\n      Nats %\n    Berlin\n88\n68\n18\n77.27%\n20.45%Boston\n175\n124\n23\n70.86%\n13.14%Chicago\n80\n63\n15\n78.75%\n18.75%London\n77\n53\n16\n68.83%\n20.78%NYC\n92\n63\n20\n68.48%\n21.74%Tokyo\n24\n22\n6\n91.67%\n25.00%\n\nKeeping in mind that this table contains races for both men and\nwomen, it looks like Tokyo has distinct winners in a higher percentage\nof races than the other majors - but that’s probably just because there\nare way fewer races. Aside from Tokyo, it looks like Berlin and Chicago\nhave higher rates of distinct winners than Boston, London, and NYC. In\nterms of distinct nationalities, it looks like though it has a slightly\nhigher rate of distinct winners than London or NYC, it has a slightly\nlower rate of distinct nationalities represented among winners.\nVery fascinating.\nAfter these descriptive angles, I’ll move more into an analysis of\nhow winning race times have changed throughout the years. There are a\nfew aspects I’ll look at, and I’ll look for both men and women, and\nacross marathons.\nThe first thing I’ll look at is winning times by marathon across\ntime, from 1980 to 2018. The data for the Boston marathon start in the\n1890s and start years for the others are scattered throughout the 1970s\nand 80s (except for Tokyo, which I guess just became a major marathon\nrecently?), so I figured I’d just start at 1980 to make it cleaner. The\nfirst figure shows men’s winning times.\n\n\n\nThe trajectory of winning times for men is surprisingly different\nacross the major marathons. What I see in this chart is that Berlin and\nLondon (and Tokyo, but harder to compare) have markedly faster courses\nand winning times have improved pretty steadily since 1980. So if you’re\nlooking to set a marathon PR in a major race, maybe choose one of them.\nBoston and Chicago, along with NYC to a lesser extent, seem to have more\nvariably slow courses. This checks out with anecdotal evidence I’ve\nheard about these races, what with Heartbreak Hill in Boston and the\nwind in Chicago. Winning times in these cities haven’t progressed in\nnearly the same way, and are even getting slower in some recent years.\nThat could be due to the field of runners, or the conditions on race\nday, or a number of things I imagine.\nLet’s look at the same figure for women now:\n\n\n\nWe see similar results to men for the women’s winning times across\nmost marathons. Berlin and Tokyo tend to be faster with more sharply\nimproving times, while Boston and NYC tend to be slower with less\nmovement on winning times. Chicago and London seem to have different\ntrajectories for women than men, though. For women, Chicago looks to\nhave more improvement in winning times, and London seems to have more\nstagnation, while it’s the opposite for men.\nTo summarize these charts, I’ll now look at the fastest winning times\nfor each marathon for both men and women.\n\n\n\nFirst, we’ll look for men:\n\n\nMarathon\n      Winning Time\n      Year\n      Winner\n      Nationality\n    Berlin\n02:02:57\n2014\nDennis Kimetto\nKenyaBoston\n02:03:02\n2011\nGeoffrey Mutai\nKenyaLondon\n02:03:05\n2016\nEliud Kipchoge\nKenyaChicago\n02:03:45\n2013\nDennis Kimetto\nKenyaTokyo\n02:03:58\n2017\nWilson Kipsang\nKenyaNYC\n02:05:06\n2011\nGeoffrey Mutai\nKenya\n\nThe table above shows the fastest winning times for each marathon for\nmen, along with who ran them in what year, and what their nationality\nis. First of all, it seems that Berlin does indeed have the fastest\ncourse of the major races, with the fastest time being 2:02:57, run by\nDennis Kimetto in 2014. Kimetto also holds the fastest time in Chicago\nwith 2:03:45. Geoffrey Mutai also shows up twice on this list, with the\nfastest times in Boston and NYC. Eliud Kipchoge and Wilson Kipsang round\nout the fastest times in London and Tokyo, respectively. Another pretty\nnoticeable feat of this table? All of the fastest times at men’s major\nmarathons are held by Kenyan runners. That’s a little more what I was\nexpecting, as I pontificated earlier.\nIt’s also worth noting that these data stop in 2018, and I’m pretty\nsure there has been some movement in these fastest times. If I remember\nright without looking it up, Eliud Kipchoge set the new world record in\nBerlin or London in 2019, with an official time of 2:01:xx. Absolutely\nridiculous.\nNow let’s look at the same thing for women:\n\n\nMarathon\n      Winning Time\n      Year\n      Winner\n      Nationality\n    London\n02:15:25\n2003\nPaula Radcliffe\nUnited KingdomChicago\n02:17:18\n2002\nPaula Radcliffe\nUnited KingdomBoston\n02:18:57\n2014\nRita Jeptoo\nKenyaBerlin\n02:19:12\n2005\nNoguchi\nJapanTokyo\n02:19:47\n2017\nSarah Chepchirchir\nKenyaNYC\n02:22:31\n2003\nMargaret Okayo\nKenya\n\nThe first thing I notice on this table is the bigger spread of\nfastest winning times across marathons - there’s over a seven minute\ndifference between the fastest and slowest best winning time, while\nthere’s only a barely over two minute difference for men. Very\ninteresting. I’m also struck by the nationalities represented with the\nUK and Japan. Anyways, Paula Radcliffe holds the two fastest times, in\nLondon and Chicago, I’m also fairly certain Radcliffe’s record was\nbroken since 2018, but I could be wrong on that one.\nWell, I could probably look at more from the data, but I feel like\nwe’ve gotten a pretty good look in here considering what’s available. I\ndo wish there were more fields, like DOB or age during the win, or more\nrunners, or something, but it’s also nice to have something a bit\nsimpler to look at. While I was searching for data I also found some\nfiles with all the Boston marathon finishers since the 1890s- I might\ntry to look into those data and see what’s there and whether I could\nwrite something up on it. Could be fun to look at distributions from\nmore runners and a lot more years. Stay tuned!\n\n\n\n",
    "preview": "posts/2022-02-07-analysis-of-major-marathon-winners/analysis-of-major-marathon-winners_files/figure-html5/time-across-years-men-1.png",
    "last_modified": "2022-08-06T15:10:11-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-09-15-notes-on-the-2020-poverty-statistics-release/",
    "title": "Notes on the 2020 poverty statistics release",
    "description": "Poverty is down from 2019 to 2020 (by the SPM) - musings on the newest batch of data on poverty and income from the Census Bureau",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2021-09-15",
    "categories": [],
    "contents": "\nOn September 14, 2021, the Census Bureau released it’s newest batch of data on poverty, income, and health insurance coverage for the year 2020. This has been a much anticipated release for poverty researchers and social scientist, as I’m sure most of you will remember that there was a pretty big global happening that upended the economy in 2020 and left everyone wondering just how bad things actually got for folks. (Quick aside: In a normal year I would absolutely have been one of those people that was anticipating the release, but this year I happen to be a new father that’s just doing his best to get by everyday, so I haven’t exactly been paying attention to when big poverty data releases are happening. I happened to see this on one of my monthly forays into the Twitterverse the day it was released, which is why I’m here now.)\nAnyways, the new poverty numbers are really fascinating and meaningful and I thought that I would do a quick post to talk about them. This will be pretty short, as I want it to be decently timely when people read it (on the off chance that anybody actually reads this).\nThe top line number that really is a big deal is that poverty actually decreased by more than 2.5 percentage points overall from 11.8% in 2019 to 9.1% in 2020. Not only that, every subgroup measured experienced a reduction in poverty.\nTake a second to let that sink in. During one of the worst recessions of the last century, in the midst of a global pandemic, poverty actually fell. That’s incredible. Let’s dive into that a little more:\nThe measure that the Census Bureau used to calculate the decrease is the Supplemental Poverty Measure (SPM), which is different than the official poverty measure that’s used to determine means-tested benefits and such. The SPM is widely considered to be a much better measure of poverty because it uses a way more reasonable measure of cost of living in its calculations, and it takes into account common household expenses like rent, taxes, medical expenses, child care, and the like. It also counts government benefits and tax credits as income, which is critical for this story.\nAlmost by nature, poverty gets worse with recessions - they’re basically part and parcel. So why did poverty actually go down in the most severe recession that we’ve seen in decades - not just stay the same, but decrease?\nThere’s actually a really simple solution here - the government gave people money. Not just people experiencing poverty, but everybody. Not only did the federal government send three rounds of stimulus checks, but it majorly beefed up unemployment and food and rental assistance and other benefits for people who desparately needed it. While the official poverty rate doesn’t capture any of these benefits (it’s based on pre-tax and transfer income and increased by several points), the SPM takes these benefits into account, and it’s clear that they had a massively positive impact. Not only did it make up for the massive loss of income that so many people experienced during COVID, it actually lifted millions out of poverty.\nThere’s so much I could say about this, but I really want to keep this short. I’ll probably end up writing more about this in future posts, as well as including some pretty plots. I’ll end this one by saying this though: The question isn’t whether we know how to solve poverty - these numbers make it clear that we know exactly how to solve poverty. The question is whether we’re only willing to do what it takes to solve poverty in a once-in-a-lifetime pandemic.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-19T13:54:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-02-test-run-with-vs-code/",
    "title": "Test Run with VS Code",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2021-08-02",
    "categories": [],
    "contents": "\nThis is a very quick test run that I’m doing to see if I can push a new blog post using VS Code.\n\n\n\n\n\n\n",
    "preview": "posts/2021-08-02-test-run-with-vs-code/test-run-with-vs-code_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-08-03T09:46:49-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-12-plug-for-the-correlates-of-state-policy-shiny-app/",
    "title": "Plug for the Correlates of State Policy Shiny App",
    "description": "A quick shout out to a really cool and valuable tool for social scientists",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2021-04-12",
    "categories": [
      "state policy",
      "no code"
    ],
    "contents": "\nI know, I know, this is my second short, no-code post in a row, but I stumbled across a really cool resource that I wanted to share with y’all.\nA few years ago a few political scientists released a massive treasure trove compendium of data on historical and modern state policy decisions for a huge amount of policies and programs across a wide variety of domains, along with associated demographic and political characteristics for state-years from 1900 to current, dubbed the Correlates of State Policy. As I was just about to start my Master of Public Policy program when I found it, of course I knew what a fantastic resource this was, and I was super pumped to dig into it. After exploring the website and associated files a bit, though, it proved to be way too clunky and overwhelming to really take the time to get to know and work with - thousands of variables and only a standard PDF codebook from what I remember. Not at all something I had time or energy to dive into and make sense of. So I forgot about it for a few years.\nBut this week I was searching the webz for fun data ideas for the blog and stumbled across a marvelous development in the Correlates of State Policy data world - a Shiny app that allows users to subset and download the data and explore visually some of the features. Cue the fireworks! It’s super cool that they have some analytics and visual capacity built in to explore the data online - that’s rad. But the thing I got super pumped about was the tool to filter and view the relevant variables and associated descriptions and download the data according to different policy domains and topics. This makes downloading the data soooo much easier than just grabbing the raw full data file and all three thousand columns. Not only can I pick the topics I want to download data for, but I can see how many variables it’ll return, and I can filter to specific states and years, to boot.\nThis is an amazing resource for social scientists or policy wonks or anyone doing broad-scale (quantitative) policy analysis. The data are super extensive and this is a really user friendly way to explore and download all that they have to offer. I will say that it’s probably wise to have the codebook pulled up as you’re viewing the variables for different policy domains. The coverage in terms of which years the variables are available for vary widely across the sources that the authors pull the data from, even within the same domain and it’s not always intuitive what data cover what years from just looking at the Shiny app. That’s really my only word of caution though. Everyone should check out this incredible resource. (That and the fact that they don’t link to the code used to develop the app.)\nIn addition to it being a great resource for academics and researchers, this is just an example of a really useful and functional Shiny app. Not only do they provide interactive graphics and user-guided analysis, but they make the underlying data really accessible, which is so clutch when the data are as unwieldy as they are in this case.\nWell, that’s all for now. I’m definitely thinking about my next substantive post for the blog, but am at a slight crossroads with what I want to focus on and spend my time doing. Do I go with more simple exploratory type analysis or do I augment my causal inference toolbox with mini causal type research questions and models or do I explore new machine learning territories that I don’t really know at all? Do I stick with data on policy topics that I work with often, or do I branch out to other data that are less serious and more for the fun of it? Do I build more shiny apps and explain them here or do I focus more on the guided analysis and insight posts that I’ve typically done? All really good things to spend my time on, and all things I want to spend time on, I just need to decide what to prioritize first and start digging in.\nUntil next time!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-12T20:45:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-05-new-shiny-app-on-world-coffee-ratings/",
    "title": "New Shiny App on World Coffee Ratings!",
    "description": "Short plug for my new Shiny app on world coffee ratings",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2021-04-05",
    "categories": [
      "shiny",
      "R",
      "coffee"
    ],
    "contents": "\nHey hey hey, folks. Hope everyone is doing swell. I wanted to take just a few minutes to do a small post about a fun little Shiny web app I just created on world coffee ratings.\nCoffee, especially the specialty coffee scene, is near and dear to my heart. Back in college and for a brief period during my first master’s I worked at a local specialty coffee shop in Fayetteville, Arkansas called Onyx Coffee Lab, and it introduced me to a world I will forever appreciate. For those that have never delved into this world, coffee is a product that is similar to wine in that there is a whole world of producers, varietals, processing methods, brewing/production methods, and even competitions that accompany it. It’s not just caffeine and dark roasts, Folgers and Starbucks - there is an entire industry dedicated to finding, procuring, roasting, and brewing the best coffees in the world.\nThis Shiny app (and post) is an homage to that world.\nI was hunting through old TidyTuesday data archives and came across a week with world coffee ratings as the featured data, and I jumped on it. It’s honestly a pretty simple web app, but I think it’s really fun. A user can select from any coffee in the database and it will pull up a polar coordinate set of its ratings along a few different dimensions, including acidity, balance, sweetness, uniformity of beans, etc, all categories that determine how good a coffee is.\n\n\n\n\nOne of the fun things that I wanted to play around with was the ggiraph package that allows you to create interactive elements in ggplot figures. That’s how I got the tooltip that shows the ratings when you hover over different rating segments. You can find the code that I used to build the app here.\nThere are a few things that I wish were different - for one, there was no unique identifier or name for the coffees in the database, and there were lots of duplicate fields for different coffees, so I had to concatenate way too many fields (clearly) to get the identifier in the drop down filter. I’m considering going back and adding filters for all of those elements individually to make it a bit easier to handle.\nAnyways, just wanted to share the latest thing I’ve been working on. Check out the app and I’ll be back soon! The next post I’m thinking about is exploring the tidymodels universe with a treasure trove of new COVID-19 data that the CDC just published. Stay tuned!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-18T16:28:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-13-analyzing-my-running-performance-using-strava-data/",
    "title": "Analyzing my 2020 running performance",
    "description": "Exploring my year in running using data from the Strava app",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2021-03-27",
    "categories": [
      "R",
      "running",
      "strava",
      "leaflet"
    ],
    "contents": "\nHello, folks! In this post I’ll be looking at my running performance over the last year by analyzing my running data from the Strava app. Before I start that, here’s a bit of background on how I’ve experienced a new-found love of running over the last year and what led to this in the first place.\nI believe I’m not the first to point out that this year has been a weird one. Societally, it’s been mostly pretty shitty (until the second week in November when things started looking up a bit). The pandemic has upended every facet of life, hundreds of thousands of people have died needlessly, there were kiler hornets, and I’m sure I’m forgetting about ten other big terrible things that happened (feel free point them out if you feel so inclined). Personally, this year has been a mixed bag for my wife and me. We’ve felt all the terrible things going on in the world at large, and our lives look so much different than we thought they would. While we’ve had our fair share of pain and grief caused by the pandemic, we were left in a relatively good spot - we both work from home in stable jobs, we were able to buy a house in a neighborhood that we love, and we were able to plan to start our family. We don’t take that for granted at all, and we truly grieve with those whose lives have been completely devastated over the last year.\nWhen the pandemic shut everything down for us starting in mid March, one of the more noticeable things to stop for us was working out at the gym. In the wake of that, my wife and I decided to start running more. At the time we were living on the upper floor in a pretty small duplex so we didn’t have a whole lot of space to do anything more than very tame bodyweight workouts inside, and we knew that we needed a physical outlet to get away from the stress of the pandemic and of me finishing my final semester of grad school without the confines of our small living space.\nEnter running. I’ve never been a complete stranger to running over the course of my life, but I’ve also never really embraced it. There have been various fits and starts of running more frequently, but it’s never been what I would call sustained. Mar had periods of more dedicated, sustainable running over the last decade or so, even training for and running a half-marathon. She could always handle distance better than I could, but I liked running faster. Since we moved to Minneapolis in 2017 up until last March, Mar and I ran an average of once every week or two probably, never more than 2 or 3 miles at a time. We liked to run around our first neighborhood here, and there were a few months where we would go run a 3 ish mile loop around a nice lake every weekend. But I’ve never exactly loved running. I’ve done it because I know aerobic exercise is good for me and because it’s a good way for Mar and I to workout together every so often, but I’ve always liked lifting and bodyweights and sprint-type workouts a lot better. Until last March, the farthest I think I’d ever run was five miles tops, probably less. I could do a mile decently fast for a normal non-runner (I think I ran a sub-six minute mile a few times in college), but I never liked running longer distances than that.\nFast forward to now, close to a year since we started that journey, I’ve run almost 1,000 miles since April, the majority of it coming in the last five or so months. Mar and I started running as a way of coping once things shutdown, but somewhere along the way it became a new thing for me, something that I genuinely started to love. We started out slow, running a mile and half or two most days, with a long run of 5.5 or 6 on Friday. Our first week of 20 miles, probably in late April or early May, was a big milestone, as was the time we ran 9 miles (thrice around our normal Friday 3 mile lake loop). June and July brought a bit of a slow-down again, with some minor injuries and tweaks, moving into our new house, and the heat of the summer. But in September I bought a new pair of running shoes (my first in probably six years) and started to get back into it again, and it was the start of a real thing for me. Mar and I still ran together from time to time, but she was in the throes of first trimester nausea, and I was increasing my mileage to 25 to 30 miles a week. I did a ten mile run in mid October at a decent pace, and that might have been what sealed it for me. In November I set out to do another 10-miler but through a weird series of events I ended up running my first half-marathon - my time was 1:50 ish, which while not competitive, is pretty good for someone who started running in earnest in March and had never run a half before. Since then I’ve been doing 30-35 mile weeks with some rest weeks here and there, and I’ve really gotten into it. I found my first running podcasts around a month ago, and started to plan my training more strategically based on what I was learning, easing my pace, incorporating strides, and dedicating one run every week or two to be more of an intervals workout.\nThe throes of winter cold and snow have descended upon Minneapolis by now, so I imagine my training will slow down a bit until mid-March or so, but all of this to say that over the last 9 months I’ve found a new love of running that will be with me for a long time I think. Once the winter thaw comes I’d like to start training for my first marathon (even if it is just by myself, but hopefully I’ll be able to find a race by then), and who knows what will come after that.\nI’ve also started to combine this new-found love of running with my love of data. In October I started downloading my running data from Strava and it’s been fun to explore it since. Today’s post will be a more formal dive into my running performance over the last year, primarily to help me see how far I’ve come and allow me to be proud of myself (something I have a really hard time with most days). Here we go! ***\nSo! To start. I’ll have two broad sections to this analysis. First I’ll do a basic descriptive look at my running performance in 2020, cutting the data by week, month, day of the week, etc. Second, I’ll make an interactive map of all my runs using leaflet so we can visualize where I run and what my typical routes look like.\nI downloaded my Strava account data from the online site, which resulted in getting a big zipped folder of a lot of different files. I know I published this post in February of 2021, but I decided to just analyze the data from 2020 to keep it simple. There are two pieces of that I incorporate here. The first is an overview file of all my activities I’ve recorded on Strava, including the name, date, time, distance, pace, etc. The second is a folder of all of the GPS location data for all of the activities, in GPX format. I’ll get back to the GPX location files here in a bit, and start with the raw activities file.\nThis code chunk loads my packages using pacman and imports and cleans the data from the activities file. Most of the cleaning is converting paces and distances into the units I generally use (miles instead of kilometers, pace in minutes per mile, etc.). Also, shoutout to the lubridate package for making the date-time parsing so painless.\n\n\n# install.packages(\"leaflet\")\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(gtools)\nlibrary(devtools)\nlibrary(leaflet)\nlibrary(gt)\nlibrary(sp)\nlibrary(maptools)\nlibrary(here)\n\n### load and clean activity summaries - no location data\nstrava <- read_excel(here(\"_posts\", \n\"2020-12-13-analyzing-my-running-performance-using-strava-data\", \n\"strava_activities.xlsx\")) %>% \n  clean_names() %>% \n  mutate(act_date = mdy_hms(activity_date), \n         act_date_time = (act_date - hours(5)),\n         month = month(act_date_time, label = T), \n         day = day(act_date_time), \n         year = year(act_date_time), \n         mdy = format(act_date_time, format = \"%m-%d-%Y\"),\n         week = floor_date(act_date_time, unit = \"week\"),\n         wkday = wday(act_date_time, label = T),\n         time = format(act_date_time, \"%H:%M:%S\"), \n         move_time = moving_time/60, \n         distance = distance_7/1.609, \n         avg_pace = move_time/distance, \n         avg_speed = average_speed/1.609, \n         max_speed = max_speed/1.609, \n         shoe = ifelse(activity_type == \"Run\" &\n                         act_date_time >= as.Date(\"2020-09-08\") & \n                         act_date_time <= as.Date(\"2020-11-15\"), \"Ghosts\", \n                       ifelse(activity_type == \"Run\" & \n                                act_date_time <= as.Date(\"2020-09-08\"), \"North Face faithfuls\", activity_gear)), \n         multiplier = distance/avg_pace, \n         performance = move_time*multiplier) %>% \n  select(act_date_time, month, day, year, mdy, week, wkday, \n         time, move_time, distance, avg_pace, avg_speed, \n         max_speed, multiplier, performance, shoe, everything())\n\n\n\nOne thing I want to point out is the creation of the multiplier and performance variables towards the bottom of the chunk. I wanted a way to compare my runs in a more standardized way that didn’t just use distance or average pace as the metric. Those are instructive indicators, but only to a certain point; I can run long distances, but if my pace isn’t where I want it to be then it doesn’t mean as much. Likewise, I can run at a fast pace, but if I can’t sustain it for longer distances, it also doesn’t mean as much. The performance metric I thought of calculates the distance divided by pace as a multiplier for how long a run is. If I run 60ish minutes for 8 miles at 8 minutes a mile, I’ll get a multiplier of around 1, which leaves my performance at ~60. As opposed to if I run 60 minutes for 10 miles at 6 minutes a mile (which I absolutely can not do), the multiplier would be 1.4ish, and the performance would be 85ish. More on this later.\nFirst things first, I want to do a basic look at my running mileage over time. I’ll start by doing a look by month, then by week.\nHere’s the monthly look.\n\n\nstrava %>% \n  filter(activity_type == \"Run\") %>% \n  group_by(month) %>% \n  summarize(mileage = sum(distance, na.rm = T)) %>% \n  ggplot(aes(x = month, y = mileage)) +\n  geom_col(fill = \"#d98004\") +\n  scale_y_continuous(limits = c(0,150), breaks = seq(0,150, by = 30)) +\n  xlab(\" \") +\n  ylab(\"Total Mileage\") +\n  labs(title = \"Cody's Monthly Running Mileage \\n\",\n       caption = \"Source: Strava\") +\n  theme_classic() +\n  theme(\n    plot.title.position = \"plot\", \n    plot.caption.position = \"plot\",\n    plot.title = element_text(size = 13)\n  )\n\n\n\n\nNow for weekly:\n\n\nstrava %>% \n  filter(activity_type == \"Run\") %>% \n  group_by(week) %>% \n  summarize(mileage = sum(distance, na.rm = T)) %>% \n  ggplot(aes(x = week, y = mileage)) +\n  geom_col(fill = \"#d98004\") +\n  scale_y_continuous(limits = c(0,50), breaks = seq(0,50, by = 10)) +\n  xlab(\" \") +\n  ylab(\"Total Mileage\") +\n  labs(title = \"Cody's Weekly Running Mileage \\n\",\n       caption = \"Source: Strava\") +\n  theme_classic() +\n  theme(\n    plot.title.position = \"plot\", \n    plot.caption.position = \"plot\",\n    plot.title = element_text(size = 13)\n  )\n\n\n\n\nThese two graphs show more or less what I explained earlier. My mileage increased throughout spring and early summer 2020, plummeted during the mid-summer, and then started to pick back up again in the fall and through the end of the year to its highest levels. It’s interesting how much the weekly look reveals that the monthly hides - even in the valley of the summer and the peaks of the fall, there’s still a lot of ups and downs. Some of that variation is the ebb and flow of my work and recovery weeks, and some of it is the weather getting harder with snow and such. Though you can’t see it, this bouncy pattern has continued into the new year. One thing I’d like to work on being more consistent with my weekly mileage - trying to ease into smoother increases and decreases in mileage rather than the stark up and downs we see here.\nObviously I knew approximately what these charts would look like. One thing I’m not as sure of how my runs break down on days of the week and time of day, both in terms of the number of runs and total mileage. This next graphs look at both the number of runs and total mileage by day of the week.\n\n\nstrava %>% \n  group_by(wkday) %>% \n  count() %>% \n  ggplot(aes(x = wkday, y = n)) +\n  geom_col(fill = \"#0c0757\") +\n  scale_y_continuous(limits = c(0,100), breaks = seq(0,100, by = 20)) +\n  xlab(\" \") +\n  ylab(\"Total Runs\") +\n  labs(title = \"Cody's Top Running Days by Total Runs \\n\",\n       caption = \"Source: Strava\") +\n  theme_classic() +\n  theme(\n    plot.title.position = \"plot\", \n    plot.caption.position = \"plot\",\n    plot.title = element_text(size = 13)\n  )\n\n\n\nstrava %>% \n  group_by(wkday) %>% \n  summarize(mileage = sum(distance, na.rm = T)) %>% \n  ggplot(aes(x = wkday, y = mileage)) +\n  geom_col(fill = \"#0d0263\") +\n  scale_y_continuous(limits = c(0,200), breaks = seq(0,200, by = 50)) +\n  xlab(\" \") +\n  ylab(\"Total Mileage\") +\n  labs(title = \"Cody's Top Running Days by Mileage \\n\",\n       caption = \"Source: Strava\") +\n  theme_classic() +\n  theme(\n    plot.title.position = \"plot\", \n    plot.caption.position = \"plot\",\n    plot.title = element_text(size = 13)\n  )\n\n\n\n\nThis is new to me! Wednesdays were my biggest days for running overall and second biggest day for mileage - I wasn’t expecting that. I was thinking Fridays might be my biggest running day, but I guess not. Saturdays were my biggest days mileage-wise, which isn’t a surprise as that’s mroe a long run day for most runners (including me), but Wednesdays got close to it. The pattern seems to have been more runs with lower mileage on Wednesdays and fewer runs with higher mileage on Saturdays. So interesting.\nNow I want to go beyond the mileage piece and look more at the performance aspect that I touched on earlier. While it’s insightful to look at distance, it’s also instructive to think about my efficiency and speed as a runner. Enter the calculation I talked about above for performance. There are a few other caveats that I want to put in here though. While the calculation I came up with (imho) is pretty good and standardizes a lot of a run, it can fall victim to extremes, either mileage wise or speed wise. Middle distance and smooth pace is more reliable. So I’ll take it with a bit of a grain of salt, and you probably should, too.\nWhat I’ll do with the performance metric is chart it across month and week, probably as a median to do a better job of smoothing outliers during those periods. I had half a mind to do a simple regression to try to pick out the best predictors of my performance but honestly, the time series nature of it would make any significant or interesting results more or less useless. I don’t do time series analysis very much and it’s hard to do right so I figure I won’t wade into it this time. (Side note, I do want to get into a bit more modelling in this blog space, but I don’t think these are the right data. Maybe I could do that for my running when I have more months’ worth of data and it’s such a clearly linear trend of increased mileage and performance.)\nHere’s a look at my median performance by month.\n\n\nstrava %>% \n  filter(activity_type == \"Run\") %>% \n  group_by(month) %>% \n  summarize(performance = median(performance, na.rm = T)) %>% \n  ggplot(aes(x = month, y = performance)) +\n  geom_col(fill = \"#d98004\") +\n  scale_y_continuous(limits = c(0,30), breaks = seq(0,30, by = 10)) +\n  xlab(\" \") +\n  ylab(\"Median Performance\") +\n  labs(title = \"Cody's Monthly Running Performance \\n\",\n       caption = \"Source: Strava\") +\n  theme_classic() +\n  theme(\n    plot.title.position = \"plot\", \n    plot.caption.position = \"plot\",\n    plot.title = element_text(size = 13)\n  )\n\n\n\n\nAfter seeing the grpahs for mileage, it shouldn’t be a surprise that my performance goes up continually once the fall settles in. Here’s the same look by week for a more granular look.\n\n\nstrava %>% \n  filter(activity_type == \"Run\") %>% \n  group_by(week) %>% \n  summarize(performance = median(performance, na.rm = T)) %>% \n  ggplot(aes(x = week, y = performance)) +\n  geom_col(fill = \"#d98004\") +\n  scale_y_continuous(limits = c(0,45), breaks = seq(0,45, by = 15)) +\n  xlab(\" \") +\n  ylab(\"Median Performance\") +\n  labs(title = \"Cody's Weekly Running Performance \\n\",\n       caption = \"Source: Strava\") +\n  theme_classic() +\n  theme(\n    plot.title.position = \"plot\", \n    plot.caption.position = \"plot\",\n    plot.title = element_text(size = 13)\n  )\n\n\n\n\nThis also checks out - not sure when the week is that I had the massive median performance. It could have been mid November when I ran my first half marathon. That would make sense.\nNow what I want to do is a map of my routes. I love maps and making them, so this is something I’m excited about it. There are a lot of things I could choose to do with this, like color the lines of my route by performance or by the date, but what I think I’ll do is keep things simple and just map the lines themselves. I run a fair few routes, but there are some that I do a lot more than others, so the map should make those lines thicker if I leave the color out of it.\nOne thing I’ll say here is that a lot of the data cleaning for the next section is taken more or less directly from other people’s code on GitHub or StackOverflow. I wouldn’t have been able to do this if a lot of way smarter people than me hadn’t done what I’m doing and been kind enough to share their code online. I’ve never worked with GPX files before, but luckily I was able to adapt code someone wrote for a Strava R package. Fun fact, my computer didn’t seem to want to just download the package, so I was left to scour the source code and bring in what I needed to work with the folder of GPX files. Here’s the setup code, including the functions to parse the GPX data.\n\n\n### function to import and process GPX strava data files for locations\n\nprocess_data <- function(path, old_gpx_format = FALSE) {\n  # Function for processing a Strava gpx file\n  process_gpx <- function(file) {\n    # Parse GPX file and generate R structure representing XML tree\n    pfile <- XML::htmlTreeParse(file = file,\n                                error = function (...) {},\n                                useInternalNodes = TRUE)\n    \n    coords <- XML::xpathSApply(pfile, path = \"//trkpt\", XML::xmlAttrs)\n    # extract the activity type from file name\n    type <- stringr::str_match(file, \".*-(.*).gpx\")[[2]]\n    # Check for empty file.\n    if (length(coords) == 0) return(NULL)\n    # dist_to_prev computation requires that there be at least two coordinates.\n    if (ncol(coords) < 2) return(NULL)\n    \n    lat <- as.numeric(coords[\"lat\", ])\n    lon <- as.numeric(coords[\"lon\", ])\n    \n    if (old_gpx_format == TRUE) {\n      ele <- as.numeric(XML::xpathSApply(pfile, path = \"//trkpt/ele\", XML::xmlValue))\n    }\n    \n    time <- XML::xpathSApply(pfile, path = \"//trkpt/time\", XML::xmlValue)\n    \n    # Put everything in a data frame\n    if (old_gpx_format == TRUE) {\n      result <- data.frame(lat = lat, lon = lon, ele = ele, time = time, type = type)\n    } else {\n      result <- data.frame(lat = lat, lon = lon, time = time, type = type)\n    }\n    result <- result %>%\n      dplyr::mutate(dist_to_prev = c(0, sp::spDists(x = as.matrix(.[, c(\"lon\", \"lat\")]), longlat = TRUE, segments = TRUE)),\n                    cumdist = cumsum(dist_to_prev),\n                    time = as.POSIXct(.$time, tz = \"GMT\", format = \"%Y-%m-%dT%H:%M:%OS\")) %>%\n      dplyr::mutate(time_diff_to_prev = as.numeric(difftime(time, dplyr::lag(time, default = .$time[1]))),\n                    cumtime = cumsum(time_diff_to_prev))\n    result\n  }\n  \n  # Process all the files\n  data <- gtools::mixedsort(list.files(path = path, pattern = \"*.gpx\", full.names = TRUE)) %>%\n    purrr::map_df(process_gpx, .id = \"id\") \n}\n\n# strava location data from GPX files\nstrava_locs <- process_data(here(\"_posts\", \n\"2020-12-13-analyzing-my-running-performance-using-strava-data\", \"activities\"))\n\n\n\nThe next tricky bit after just reading in the GPX files was figuring out how manipulate them into line segments rather than points, so I could map my routes. Luckily, Kyle Walker has a great post on doing just that and created a handy function I was able to use, shown below.\n\n\n### function to clean points data to lines for easier mapping\n\npoints_to_line <- function(data, long, lat, id_field = NULL, sort_field = NULL) {\n  \n  # Convert to SpatialPointsDataFrame\n  coordinates(data) <- c(long, lat)\n  \n  # If there is a sort field...\n  if (!is.null(sort_field)) {\n    if (!is.null(id_field)) {\n      data <- data[order(data[[id_field]], data[[sort_field]]), ]\n    } else {\n      data <- data[order(data[[sort_field]]), ]\n    }\n  }\n  \n  # If there is only one path...\n  if (is.null(id_field)) {\n    \n    lines <- SpatialLines(list(Lines(list(Line(data)), \"id\")))\n    \n    return(lines)\n    \n    # Now, if we have multiple lines...\n  } else if (!is.null(id_field)) {  \n    \n    # Split into a list by ID field\n    paths <- sp::split(data, data[[id_field]])\n    \n    sp_lines <- SpatialLines(list(Lines(list(Line(paths[[1]])), \"line1\")))\n    \n    # I like for loops, what can I say...\n    for (p in 2:length(paths)) {\n      id <- paste0(\"line\", as.character(p))\n      l <- SpatialLines(list(Lines(list(Line(paths[[p]])), id)))\n      sp_lines <- spRbind(sp_lines, l)\n    }\n    \n    return(sp_lines)\n  }\n}\n\n\n\nAfter that, I can convert my points data to lines, below.\n\n\nstrava_locs_clean <- strava_locs %>% \n  as_tibble() %>% \n  clean_names() %>% \n  rename(time_old = time) %>% \n  mutate(act_date = ymd_hms(as.character(time_old)), \n         act_date_time = (time_old - hours(5)),\n         month = month(act_date_time), \n         day = day(act_date_time), \n         year = year(act_date_time), \n         mdy = format(act_date_time, format = \"%m-%d-%Y\"),\n         week = week(act_date_time),\n         wkday = wday(act_date_time, label = T),\n         time = format(act_date_time, \"%H:%M:%S\"))\n\nstrava_lines <- points_to_line(\n  data = strava_locs_clean, \n  long = \"lon\", \n  lat = \"lat\", \n  id_field = \"id\", \n  sort_field = \"cumtime\"\n)\n\nline_match_run <- strava_locs_clean %>% \n  filter(cumtime == 0) %>% \n  left_join(strava, by = \"act_date_time\") %>% \n  arrange(id)\n\n\n\nNow I need to join the spatial lines table (strava_lines) back in with the rest of the fields for the activities to create a spatial lines data frame. This is mainly so I can filter down to just runs and get the hikes and walks out of there. (I also have a few activities that are labelled as runs that are actually when I was playing tennis, so I’ll get those out of there too.) The reason I do subset instead of dplyr’s filter is because the spatial lines data frames don’t work well with tidy verse. I suppose I could have filtered the rows before joining and just done an inner join, but whatever.\n\n\nstrava_sldf <- SpatialLinesDataFrame(strava_lines, line_match_run, match.ID = F)\n\nstrava_run_sldf <- subset(strava_sldf, strava_sldf$activity_type == \"Run\" & \n                            !grepl(\"tennis\", strava_sldf$activity_name, ignore.case = T))\n\n\n\nNow we’re ready to map! My go-to package for mapping is leaflet - so simple and intuitive to build and there are some great features. I won’t build too complicated of a map here, but I’m sure I’ll make more advanced maps in future posts. Here I’m choosing to zoom my map to northeast Minneapolis because that’s where the majority of my runs originate. However, I have run in a few other places and recorded on Strava, like a few trails in some Twin Cities suburbs, and a few in Grand Marais, on the north shore of Lake Superior.\nThe great thing about leaflet maps is that they are interactive, though! So you can pan to different parts of the city to follow routes, and you can zoom in and out to look at routes more closely. You should also zoom out a bit and see if you can find those other routes that aren’t in Minneapolis, too!\nThe lines are darker when there are more routes there, and lighter when there are fewer routes. Zooming in really close to the lines will show you just how many routes are there.\n\n\nmap <- leaflet::leaflet(data = strava_run_sldf) %>% \n  addProviderTiles(providers$CartoDB.Positron) %>%\n  addPolylines(weight = 3) %>% \n  setView(-93.2474, 45.0132, zoom = 12.5)\n\nhtmlwidgets::saveWidget(map, here(\"_posts\", \n\"2020-12-13-analyzing-my-running-performance-using-strava-data\", \"strava_map.html\"))\n\n\n\n\n\n\n\nBut this is all for now. I could do a lot more with this, but honestly, I just want to publish it. I’ve been working on it for several months now and I’ve only just worked up the motivation to finish it. So here it is! Woo!\n\n\n\n",
    "preview": "posts/2020-12-13-analyzing-my-running-performance-using-strava-data/analyzing-my-running-performance-using-strava-data_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-08-05T17:39:30-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-12-07-a-true-welcome/",
    "title": "A true welcome",
    "description": "Why I'm here",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2020-12-07",
    "categories": [
      "no code",
      "intro"
    ],
    "contents": "\nGreetings, esteemed guest!\nI’m so glad you made it to my site. Whether you’ve known me for a long time and you’re only here because I asked you to check it out, or we’ve never met and you stumbled across this somehow, thanks for being here. I know that there are a lot of other places on the internet you could be and a lot of other things you could be doing with your time, so that you’ve chosen to spend a few minutes here is something I don’t take lightly. Thanks.\nI wanted to take a few minutes to jot down some thoughts about why I started this and what I’m doing here. As you probably figured out by now, this is my site where I blog about data and public policy. I’m a research scientist and policy analyst by trade, and I love working with data - this is my creative outlet to explore the areas that I’m passionate about and get better at my craft. Exploring new data sets about topics that I care about is really exciting to me, because I get to both learn a lot and help people out in the process. One of the ways that I learn best is by following along as people show me how to do something, and that applies to data science especially. I love finding new data and R blogs to follow and explore, particularly when they’re at a skill level that’s close to mine or just slightly more advanced. Hopefully this site can be that kind of a resource to people who are interested in improving their data tool box.\nOld site\nThis site is really a reincarnation of a previous blog that I started back in the summer of 2019. I was on break from my MPP program at the time, only doing an internship and no classes, and I wanted to do something productive with my down time that would help my job prospects once I graduated, either in terms of actual experience and skill-sharpening or just a portfolio to point to. Over the course of the summer I really started to enjoy the process of exploring new data sets and writing about both my process of analaysis and visualization and also the insights and thoughts I was pulling from the analysis. I did circa 10 posts over the course of around three months, most of them going through new data sets in R, but also some posts about Tableau public dashboards I’d made or with my thoughts about a particular policy area that I’d been thinking about a lot. After school started back up again in the fall I wasn’t able to keep it up, but I did do a few more posts during the winter break between fall and spring semesters. Since I graduated in May 2020, I’ve been meaning to pick it back up again, but time the last six months has gotten away from me, with starting a new full-time job, buying a house, finding out that my wife and I are expecting our first child, and general COVID craziness. I also knew that I wanted to create a new site, which would take some planning and investment on top of the time to actually write new posts. My old site, while functional and serviceable, didn’t look that great and it was not at all integrated with the rest of the writing and analysis workflow I’ve developed in RStudio and R Markdown.\nNew site!!!\nEnter this new site! At various points over the last year, I’ve seen posts and resources on various ways to start blogs using R packages, like blogdown and distill, but I wasn’t able to really sit down and start thinking about it until now. I tried starting a site using blogdown and Hugo, and while I’m sure it’s a great tool that works for a lot of folks, I wasn’t able to get it to work. One of the other posts that I’d seen, though, was this one from Tom Mock about starting a blog using distill and Netlify - it worked a lot more seamlessly for me than did the blogdown approach, and now we find ourselves here!\nMostly, I see this new site as a better-looking and easier-to-work-with continuation of the old site in terms of content and purpose. I’m really excited to get back to exploring new data sets and getting better at my analysis and visualization skills, so I imagine that will by main focus for the first stage in this new site. I’d also like to explore new territory with this blog, though. There are a few domains that I’ve been really itching to get into, like machine learning, interacative web graphics using d3 and JavaScript, and I’d love to use this as a way to explore them. I’d also like to create more Shiny web apps and blog about them here. Currently I have one Shiny app that I made for a class, and I’ve been meaning to build more - it’s a super fun platform.\nRight now I’m thinking that the first new post that I do here will be analyzing my Strava app running data. I figured out how to download it a few weeks ago and I’ve had a good time playing around with it. I’m also debating building a Shiny app for it, but that remains to be seen. Other than that, I’m wide open to new data sets and topics.\nWell, folks, that’s all for now. If you want to learn more about me, check out the “About” page on the site. Thanks for stopping by and taking the time to read this - I’m looking forwrad to posting more soon!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-12T20:59:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-02-cfpb-analysis/",
    "title": "CFPB Analysis",
    "description": "A repurposing of a post I did on my old blog site last January. Wanted to get some content up here to see how the workflow goes.",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2020-12-02",
    "categories": [
      "R",
      "CFBP",
      "equity",
      "banking"
    ],
    "contents": "\n*Update: repurposing this post for my new blog site! will actually\nhave a new post detailing catching everyone up on everything soon\nHello everyone! Welcome back to my blog - it’s been way too long, I\nknow, but I’m back for a post before I start my new semester (my final\nsemester of grad school HALLELUJAH). As much as I want to update you on\nmy life and everything, I’m going to skip the pleasantries and just get\nstraight to the post. I don’t really have time to get into all that I\nwant to with the data as it is, so I probably shouldn’t take the time to\nupdate you on my life when you probably wouldn’t read it anyways.\nIn this post I’m going to be looking at data from the 2016\nNational Financial Well-Being Survey, published by the Consumer\nFinancial Protection Bureau. As you can probaby guess, it’s a survey\nabout people’s financial well-being, with questions about financial\nskills, behaviors, attitudes, experiences, etc., as well as demographics\nlike race, employment, poverty status, urban/rural, etc. Since interning\nat the Federal Reserve Bank last summer, I’ve definitely gained more of\nan interest in financial issues and credit and such, especially racial\ninequities within access to credit and financial services. So that’s\nwhat I’ll be looking at in this post. There’s a lot in these data that I\nwon’t have time or space to go into wtih this post that I’d like to -\nurban/rural differences, gender differences, educational differences,\nregional differences, not to mention all of the financial measures that\nI won’t get to look at.\n(As a quick plug, I found the data from a site called Data is Plural, which\nhas a spreadsheet of hundreds of interesting data sets, updated every\nweek. Check it out.)\nThe main thing that I want to look at in this post is racial\ndifferences in access to credit, and whether/how that can be explained\nby poverty status and financial well-being and financial skill. One of\nthe central features of banking and lending history in America is racial\ndiscrimination. Practices such as redlining\nshaped the current urban housing landscape, and though now illegal,\ndiscriminatory lending practices are still rampant. Several years ago\nthere was a decently prominant story\nby Reveal News discussing how people of color and minorities are\nmore likely to be turned down for a loan than whites across many metro\nareas. Defenders of this dynamic say that its justified by lower credit\nscores for minorities, and insist that black people are just worse with\nmoney and aren’t as credit worthy. While credit scores aren’t public\ndata and therefore can’t be explored as factor in racial disparities in\nlending, studies\nhave shown that credit score algorithms have a discriminatory impact on\npeople of color. So even if credit scores do account for differences in\nlending, that doesn’t explain away discrimination.\nWith that, let’s get into the data.\nData Loading and Survey\nWeights\nHere I simply load in my data (already mostly cleaned thanks to a\nhandy R script provided by CFPB) and set it up as a survey. I decided to\nuse the srvyr package, which lets you work with survey\ndataframes in tidyverse style. After adding the survey weights in the\ncode below, we should be good to go.\n\n\ndata <- read.csv(\"/Users/codytuttle/Desktop/cfpb_blog/cfpb_clean.csv\")\nlibrary(tidyverse)\nlibrary(survey)\nlibrary(srvyr)\nweighted <- data %>%\n  as_survey(PUF_ID, weight = finalwt) %>%\n  rename(race = PPETHM, \n         incomecat = PPINCIMP) %>%\n  mutate(rejected = ifelse(REJECTED_1 == \"Yes\", 1, \n                           ifelse(REJECTED_1 == \"No\", 0, 99)))\n\n\nAccess to Credit\nThe first thing that I want to do is look at overall racial\ndisparities in access to credit using a survey question that asks\nwhether a respondent applied for credit and was turned down in the last\nyear.\n\n\nweighted %>%\n  group_by(race) %>%\n  filter(rejected != 99) %>%\n  summarize(rejected = survey_mean(rejected)) %>%\n  ggplot(aes(x = race, y = 100*rejected, group = race)) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    theme_minimal() +\n    labs(x = \"Racial Group\", y = \"% rejected\") +\n    ggtitle(\"Credit Rejection by Racial Group, 2016\")\n\n\n\nThis chart shows astounding racial disparities in lending - black\nrespondents were over twice as likely to report being turned down for a\nloan in the past year than whites, and Hispanics were nearly 50% more\nlikely than whites.\nHowever, while these differences are stark, in and of themselves they\ndon’t necessarily constitute discrimination. There are certain\ncharacteristics not accounted for in that graph, primarily income, that\ncould account for the observed differences. To take one step further in\nexploring whether or not these disparities could be evidence of\ndiscrimination, below I chart the same measure of being rejected for a\nloan by both income category and race. I limit the racial categories to\njust black and white, both to focus on the starkest disparities and to\nminimize clutter on the graph.\n\n\nweighted %>%\n  filter(race == \"Black, Non-Hispanic\" | race == \"White, Non-Hispanic\", rejected != 99) %>%\n  group_by(race, incomecat) %>%\n  summarize(rejected = survey_mean(rejected)) %>%\n  ggplot(aes(x = incomecat, y = 100*rejected, group = race, fill = race)) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1)) +\n    labs(x = \"Income Category\", y = \"% rejected\", fill = \"Racial Group\") +\n    ggtitle(\"Credit Rejection by Income Category and Racial Group, 2016\")\n\n\n\nHere we can see that the racial disparities in lending observed in\nthe first chart persist even after controlling for income, and may even\nbe starker. Across every single income category, black respondents are\nrejected at a higher rate than white respondents, generally at a two\nto three times higher rate, at least. If the decision to lend is\nabout the borrower’s ability to pay it back, then income should be a\npretty good metric. But even holding income equal, there are still\nmassive racial disparities in lending.\nHowever, givng them the benefit of the doubt, maybe lenders are\npicking up on some other factor besides race that are causing them to\nreject disproportionately more black folks. In the next graph I look at\nthe financial skills score by race and by income category and race. Even\nif it’s not a measure that’s directly used by lenders, it still probably\ncan be a proxy for overall creditworthiness.\n\n\nweighted %>%\n  filter(race == \"Black, Non-Hispanic\" | race == \"White, Non-Hispanic\") %>%\n  group_by(race) %>%\n  summarize(finskill = survey_mean(as.numeric(FSscore))) %>%\n  ggplot(aes(x = race, y = finskill)) +\n    geom_bar(stat = \"identity\", fill = \"blue\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1)) +\n    labs(x = \"Race\", y = \"Financial Skills Score\") +\n    ggtitle(\"Financial Skills Score by Race, 2016\")\n\n\nweighted %>%\n  filter(race == \"Black, Non-Hispanic\" | race == \"White, Non-Hispanic\") %>%\n  group_by(race, incomecat) %>%\n  summarize(finskill = survey_mean(as.numeric(FSscore))) %>%\n  ggplot(aes(x = incomecat, y = finskill, group = race, fill = race)) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1)) +\n    labs(x = \"Income Category\", y = \"Financial Skills Score\", fill = \"Racial Group\") +\n    ggtitle(\"Financial Skills Score by Income Category and Race, 2016\")\n\n\n\nWell, it doesn’t look like giving lenders the benefit of the doubt is\nworking out very well - both overall and across each income category,\nblack respondents actually have very similar financial skills scores as\nwhites, if not higher.\nBear with me, because now I’m about to get a bit more weedsy. In the\nnext few graphs I’m going to really try and get to the bottom of this\nissue as best as I can using these data and without using any fancy\nstatistical techniques (this is neither the time nor the place for\nthat). These charts will look at the financial skills scores of the\nfollowing groups: all rejected applicants; all accepted applicants;\nrejected white applicants vs. accepted black applicants; and rejected\nblack applicants vs. accepted white applicants.\n\n\nweighted %>%\n  filter(race == \"Black, Non-Hispanic\" | race == \"White, Non-Hispanic\", rejected == 0) %>%\n  group_by(race, incomecat) %>%\n  summarize(finskill = survey_mean(as.numeric(FSscore))) %>%\n  ggplot(aes(x = incomecat, y = finskill, group = race, fill = race)) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1)) +\n    labs(x = \"Income Category\", y = \"Financial Skills Score\", fill = \"Racial Group\") +\n    ggtitle(\"Financial Skills Score of Rejected Credit Applicants by Income and Race, 2016\")\n\n\nweighted %>%\n  filter(race == \"Black, Non-Hispanic\" | race == \"White, Non-Hispanic\", rejected == 1) %>%\n  group_by(race, incomecat) %>%\n  summarize(finskill = survey_mean(as.numeric(FSscore))) %>%\n  ggplot(aes(x = incomecat, y = finskill, group = race, fill = race)) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1)) +\n    labs(x = \"Income Category\", y = \"Financial Skills Score\", fill = \"Racial Group\") +\n    ggtitle(\"Financial Skills Score of Accepted Credit Applicants by Income and Race, 2016\")\n\n\nweighted %>%\n  filter((race == \"Black, Non-Hispanic\" & rejected == 0) | \n           (race == \"White, Non-Hispanic\" & rejected == 1)) %>%\n  group_by(race, incomecat) %>%\n  summarize(finskill = survey_mean(as.numeric(FSscore))) %>%\n  ggplot(aes(x = incomecat, y = finskill, group = race, fill = race)) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1), \n          plot.title = element_text(size = 10)) +\n    labs(x = \"Income Category\", y = \"Financial Skills Score\", fill = \"Racial Group\") +\n    ggtitle(\"Financial Skills Score of Rejected White Credit Applicants vs. \n            Accepted Black Applicants by Income, 2016\")\n\n\nweighted %>%\n  filter((race == \"Black, Non-Hispanic\" & rejected == 1) | \n           (race == \"White, Non-Hispanic\" & rejected == 0)) %>%\n  group_by(race, incomecat) %>%\n  summarize(finskill = survey_mean(as.numeric(FSscore))) %>%\n  ggplot(aes(x = incomecat, y = finskill, group = race, fill = race)) +\n    geom_bar(stat = \"identity\", position = \"dodge\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1), \n          plot.title = element_text(size = 10)) +\n    labs(x = \"Income Category\", y = \"Financial Skills Score\", fill = \"Racial Group\") +\n    ggtitle(\"Financial Skills Score of Rejected Black Credit Applicants vs. \n            Accepted White Applicants by Income, 2016\")\n\n\n\nHere we go: First, for all rejected credit applicants, we see the\nsame pattern of black respondents having similar or slightly higher\nfinancial skills scores than white respondents. Second, the pattern\nholds for all accepted applicants. No surprises there.\nThird, when we look at rejected black applicants vs. accepted white\napplicants, in order for there to be any justification of disparities at\nall, rejected black repondents should have clearly lower financial\nskills scores than accpeted white applicants. We do see this across\nseveral income categories, but it doesn’t at all appear to be the norm -\nwe see very similar financial skills between rejected blacks and\naccepted whites for three categories, and rejected blacks even have\nmoderately higher financial skills scores than accepted whites\nfor one income category.\nFourth, in order to justify the somewhat similar or even higher\nfinancial skills scores for rejected black applicants vs. accepted\nwhites, we should see a similar pattern for rejected whites vs. accepted\nblacks (i.e., sometimes rejected whites have similar or higher financial\nskills than accepted blacks). However, we don’t see this pattern at all\n- across every income bracket, accepted black applicants have far and\naway higher financial skills scores than rejected whites.\nIn other words, the decision to reject a white applicant\nvs. accepting a black applicant with a similar income is clearly\njustified, at least in terms of financial skills, while the same cannot\nnecessarily be said for rejecting a black applicant. To me, this\nsuggests that black applicants for credit are held to a much higher\nstandard than white appliants with similar credentials. In order for the\nobserved disparities in credit rejection to be justified, there would\nhave to be very clear differences between the many black applicants that\nare rejected and the whites that are accepted. However, we don’t see\nthat - the disparities persist even after controlling for income, and\noften times financial skills as well.\nThis to me is evidence that the observed racial disparities in\nlending are indeed indicative of discrimination. At the very least, even\nif not outright discrimination, it’s clear that black applicants are\nheld to a much higher standard in credit decisions than whites with\nsimilar incomes. It would be foolish of me to claim that this analysis\nrepresents any kind of causal or definitive proof of discrimination -\nthis is just one measure based on a survey that doesn’t account for\nactual credit score. But in my view, it represents one more piece of\nevidence that across so many sectors, banking and credit included, black\npeople and people of color are not treated the same as whites.\nAs always, I welcome your questions, comments, and feedback. I doubt\nI’ll be able to post again until this summer, but I’m really glad I was\nable to do this before my semester started again. Until next time!\n\n\n\n",
    "preview": "posts/2020-12-02-cfpb-analysis/cfpb-analysis_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-08-10T19:19:21-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
