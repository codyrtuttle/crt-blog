[
  {
    "path": "posts/2023-05-01-minneapolis-neighborhoods/",
    "title": "Minneapolis neighborhoods part 1",
    "description": "A brief exploration of Minneapolis neighborhood characteristics",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2023-05-02",
    "categories": [
      "neighborhoods",
      "leaflet",
      "r",
      "geospatial"
    ],
    "contents": "\nA new code post!\nFinally!\nHey folks! I’m finally back with another code post at long last. Since I got my new computer I’ve been itching to get back into blogging and I have to some extent with the new musings blog and such. But what I really have been wanting is a new code post. Like I said in a few recent short entries both here and in my musings blog, I just don’t know what to do. So I made the executive decision today to just do something that I’ve been exploring for a bit and have some code already done for.\nAnd that something is a brief analysis of Minnepaolis neighborhoods! I downloaded some data from the MN Compass a few months ago to start exploring for the blog and now I’m finally deciding to do something with it. There’s honestly a lot I could do with these data, and I imagine I will do more in the future, but for now I’m going to keep it simple with three different maps showing how Minneapolis neighborhoods differ across a few socio-demographic and economic characteristics.\nData\nFirst I’ll read in (and explain a bit about) the data sources. I downloaded shapefiles for Minneapolis neighborhoods in the form a geojson from the City of Minneapolis website open data portal. They have different forms available, so check it out here.\nI got the measures themselves from MN Compass, which is a fantastic resource for Minnesota community data. They have a Minneapolis - St. Paul Neighborhood Profiles page where you can download neighborhood data. Unfortunately the data are bit outdated being from 2019 (with some measures being even older than that), but beggars can’t be choosers. This is a treasure trove with over 300 columns of data for each neighborhood.\nBelow I load my packages and data. I use the geojson_sf() function from the geojsonsf package to read in the Minneapolis neighborhoods shapefile, but I imagine there are probably plenty of other ways to read in geojson data.\n\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(here)\nlibrary(geojsonsf)\nlibrary(readxl)\nlibrary(ggiraph)\nlibrary(glue)\nlibrary(janitor)\n\noptions(scipen=999)\n\nmpls_geojs <- geojson_sf(\n  here(\n    \"data\", \n    \"Minneapolis_Neighborhoods.geojson\"\n  )\n) %>% clean_names()\n\nmpls_data <- read_excel(\n  here(\n    \"data\", \n    \"neighborhoods_msp_2019_with_suppression.xlsx\"\n  )\n) %>% clean_names()\n\n\nNext I do some cleaning of the neighborhood characteristics data to get it into a shape to join with the shapefile, mainly cleaning neighborhood names to match across datasets. Honestly, that involved a lot of trial and error and manual inspection to see which names were different between the two either in spelling/convention or in an actual neighborhood name change (like Calhoun to Bde Maka Ska). After that I join the two together to create a full dataset with the geometry included. I think technically it creates an sf object but for the purposes of this analysis (and I think most applications), you can manipulate it just the same way as a normal tibble or data frame.\n\n\nmpls_data_clean <- mpls_data %>% \n  mutate(\n    neighborhood = case_when(\n      str_detect(neighborhood, \"(Minneapolis)\") ~ \"Como\",\n      str_detect(neighborhood, \"Near\") ~ \"Near - North\", \n      str_detect(neighborhood, \"Cedar-\") ~ \"Cedar - Isles - Dean\",\n      str_detect(neighborhood, \"West Calhoun\") ~ \"West Maka Ska\",\n      str_detect(neighborhood, \"-\") ~ str_replace(neighborhood, \"-\", \" - \"),\n      T ~ neighborhood\n    )\n  )\n\nmpls_full <- mpls_geojs %>% \n  left_join(mpls_data_clean, by = c(\"bdname\" = \"neighborhood\")) \n\n\nMaps!\nAnd now, with some very minimal cleaning and joining, we can start mapping! I’ll create three interactive maps each showing different neighborhood characteristics using the ggiraph package and functionality. I love this package - super easy to use and lots of control and customization and cool features to make quick and easy interactive plots (and even maps!). I’ve tried theplotly package a few times and I’ve always had better success wtih ggiraph Shout outs to the colleague that showed this to our team a few years ago!\nTotal population\nFirst things first - let’s start with a simple map of the total population by neighborhood to get an idea of how comparable they are in terms of the base number of residents.\n\n\nmap_pop <- mpls_full %>% \n  ggplot() +\n  geom_sf_interactive(\n    aes(\n      fill = total_population, \n      tooltip = glue(\n        \"Neighborhood: {bdname} \\nTotal Population: {total_population}\"\n      )\n    )\n  ) +\n  labs(\n    title = \"Minneapolis neighborhoods by total population\", \n    fill = \"Total population\"\n  ) +\n  scale_fill_gradient(labels = scales::comma) +\n  theme_void() +\n  theme(\n    plot.title.position = \"plot\"\n  )\n\ngirafe(ggobj = map_pop)\n\n\n\nInteresting! It looks like Whittier, Marcy Holmes, and Prospect Park are the three most populous neighborhoods in the city, with over 14,000 residents. By contrast, my neighborhood of Waite Park in the northeast corner has a medium-low population compared to others. Other than that it looks like there’s a good mix of more and less populated neighborhoods throughout the city.\nChildren under 5\nBecause I have a child under 5, the next thing I’m interested in plotting is the share of the population that’s under age 5 in each neighborhood.\n\n\nmap_u5 <- mpls_full %>% \n  ggplot() +\n  geom_sf_interactive(\n    aes(\n      fill = under_5_years_share, \n      tooltip = glue(\n        \"Neighborhood: {bdname} \\n% Under 5: {round(under_5_years_share*100, 1)}%\"\n      )\n    )\n  ) +\n  labs(\n    title = \"Minneapolis neighborhoods by share of population under age 5\", \n    fill = \"Share under 5\"\n  ) +\n  scale_fill_gradient(labels = scales::percent) +\n  theme_void() +\n  theme(\n    plot.title.position = \"plot\"\n  )\n\ngirafe(ggobj = map_u5)\n\n\n\nThis is also really interesting - it looks like there’s a handful of neighbohorhoods with relativley high populations under 5. Cedar-Riverside and Harrison both have high shares under 5 at over 12%, and Ventura Village and Powderhorn Park are over 11%. No surprises that the areas around the university have the lowest population of littles. Again, my neighborhood has a relatively low share under 5 - I honestly thought it would be higher.\nPoverty\nThe last thing I want to look at, at least for now, is the poverty rate in each neighborhood. As you all probably know, poverty is one of my primary research interests, so this will be fascinating for me.\n\n\nmap_pov <- mpls_full %>% \n  ggplot() +\n  geom_sf_interactive(\n    aes(\n      fill = with_income_below_poverty_share, \n      tooltip = glue(\n        \"Neighborhood: {bdname} \\n% Poverty: {round(with_income_below_poverty_share*100, 1)}%\"\n      )\n    )\n  ) +\n  labs(\n    title = \"Minneapolis neighborhoods by poverty rate\", \n    fill = \"Poverty rate\"\n  ) +\n  scale_fill_gradient(labels = scales::percent) +\n  theme_void() +\n  theme(\n    plot.title.position = \"plot\"\n  )\n\ngirafe(ggobj = map_pov)\n\n\n\nAgain - super fascinating. I’m going to ignore the areas around the university again, as poverty among college kids is kind of a different thing. Barring them, it looks like near North neighborhoods (Near North itself, Harrison, Hawthorne) and near South neighborhoods (Cedar-Riverside, Ventura Village, Phillips West) all have poverty rates hovering around 35-45%, which is really high relative to farther West and Southwest and farther Southeast and farther Northeast. What’s more sobering for me is that the neighborhoods with really high rates of kids under 5 (Cedar-Riverside and Ventura Village) both have really high poverty rates.\nClosing thoughts\nAs much as I want to do more, I’m going to leave it here for now. These are just three of the literally over 300 measures that the MN Compass data provides, so there is SO much more that I could show here, and I definitely might in a future post. But for now, I want to keep it simpler and just post something.\nIn the course of searching for interesting Minneapolis neighborhood data, I found that the City open data portal that I mentioned earlier is honestly a great resource for some interesting data. I found some geo-located individual crime data that I might map, and I’m also thinking about doing some modelling to see what kinds of neighborhood characteristics predict crime rates - I’ll have to think carefully about that one, as I don’t want to present it carelessly and portray any neighborhoods negatively.\nSee ya later!\n\n\n\n",
    "preview": {},
    "last_modified": "2023-05-02T20:38:51-05:00",
    "input_file": "minneapolis-neighborhoods.knit.md"
  },
  {
    "path": "posts/2023-04-20-new-musings-blog/",
    "title": "New musings blog",
    "description": "A plug for my new project",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2023-04-20",
    "categories": [
      "no code"
    ],
    "contents": "\nThis will be another short post with no code, but I wanted to plug the new musings style blog that I made! At this point, it was a way to get some experience creating a blog with Quarto, but I envision being a supplement to this primary site with shorter posts, not necessarily with code, and not necessarily long or well thought-through. Just a way to write more and hopefully get more content out there. Anyways, the new blog is here - go check it out! I also linked to it on the nav bar from this main site.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-20T20:38:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-09-i-got-a-new-computer/",
    "title": "I got a new computer!",
    "description": "A few thoughts on setting it up",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2023-04-09",
    "categories": [
      "no code"
    ],
    "contents": "\nHey folks!\nI just got a new computer, so I’m all jazzed again about blogging. I’d been working with a MacBook Air from 2015 that was just pretty old, and upgraded to a MacBook Air M1 model. It’s nice! I’m still getting used to a new set up and trying to get R and everything like that installed again, which is testing my pateince, but I know it’ll all come. I’m debating building a new supplementary site with short musings and such in Quarto just to try out something different. If I do that, the plan wouldn’t be to forget this site - it’d still be my main site. It would just be a way to try something different and try to get myself back into a blogging/coding for fun space, which I haven’t been in a pretty long time.\nAll this to say, hopefully this is the start of a bit more regular posting for me, either here or in a new “musings” style site from time to time.\nThanks, folks!\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-09T20:07:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-13-thoughts-on-poverty/",
    "title": "Thoughts on poverty",
    "description": "Raw and unfiltered thoughts from the new Census poverty numbers.",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2022-09-13",
    "categories": [
      "no code",
      "poverty"
    ],
    "contents": "\nHey folks - I don’t have much time to post right now, but in the\nspirit of posting more frequently and being less of a perfectionist, I’m\ngoing to write for 15 minutes about my current musings and thoughts on\npoverty. The new poverty numbers from the Census Bureau came out today,\nand they’ve got me somewhere in between my head and my heart reflecting\non the dynamics of poverty - the intellectual and scholarly\nunderstanding, the political economy and societal optics, the lived\nreality and experience of people in poverty. With all of this, I’m just\ngoing to take 15 minutes and just get my thoughts down in writing. Like\nI alluded to, this is probably going to be something in between\nintellectual/academic musings and personal/emotional reflection. But\nwhatever it ends up actually being, it’s the honest and raw thoughts\nthat are swirling in my brain right now. I hope to find the time soon to\ndo a little more digging into these reflections, both from a conceptual\nand philosophical perspective, but also the data and research.\nHere it goes:\nI will start out with an obvious but I think important qualification\non all of these thoughts: I am a very privileged white male living in an\nupper-middle class neighborhood with a poverty rate of next to nothing.\nI have never experienced real poverty aside from being a broke college\nstudent. I didn’t experience poverty growing up as a kid, nor have I\nexperienced poverty as an adult. I live in a decently large city, so I\nsee poverty around me fairly frequently, but I am not residentially- or\nsocially-proximal to poverty in the way that I’m reflecting on now.\nThe new poverty numbers have me considering how complex and dynamic\npoverty truly is. At a very top line, without actually reading into the\ndetails just yet, the Census Bureau released the poverty statistics for\n2021, and poverty by most conceptions is down considerably from 2020,\nespecially child poverty. This is a true dynamic, and it’s a good thing,\nfull stop. But the official poverty numbers, whether in the form of the\nofficial poverty measure or the supplemental poverty measure, do not\neven come close to really capturing the experience of people living in\nfinancial and material deprivation.\nOfficial statistics, whether absolute (like the official poverty\nmeasure) or relative (like the supplemental poverty measure) are based\non hard thresholds and cutoffs for who falls into the “poverty” category\nand who does not, because they have to be. But what these cutoffs miss\nis the experience of people who are just beyond that bright line -\npeople who have income a little higher than whatever threshold is being\nused, but are actually no better off than the folks just below it. So\nmuch policy, so many benefits, so much scholarship and research, is\nbased on these arbitrary lines, and it leaves out the real experience of\nso many people. So many families are denied benefits because they have\nincome a few dollars higher than the poverty criteria being used,\nbenefits that the new numbers and years of evidence show actually help\nfamilies improve their circumstances and life prospects.\nAnother thing is that these numbers, especially the official poverty\nmeasure, are based on calculations taken at one point during the year.\nThey’re a sampling of folks who were above and below the cutoff at one\npoint during the year. But this completely missed the inherent dynamic\nnature of poverty. Lots of research shows that it’s just really really\neasy to fall into poverty. Most families and individuals are just one\nemergency or accident away from slipping into devastating financial\ncircumstances that could completely redefine their entire life\ntrajectory. My family is just one emergency\naway from this.\nAnd yet. Something that the new numbers also miss is that communities\nof color and other historically and currently marginalized groups are\nso much more likely to experience poverty than white folks. The\ntruth is that my family and I are most likely able to weather an\nemergency or shock a lot more easily than another family that looks just\nlike us but are Black, simply because of the societal and generational\nprivilege we are privy to.\nPoverty is complex and dynamic. There are so many ways of knowing\nthat we need to tap into - not just the official (or supplemental)\nstatistics, but deeper quantitative analysis that looks at distributions\nand timing, and most importantly, real qualitative lived experience of\nfolks in and near poverty.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-08T20:04:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-10-nyt-bestsellers/",
    "title": "NYT bestsellers",
    "description": "An exploration of NYT bestselling authors using {reactable}.",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2022-08-10",
    "categories": [
      "R",
      "reactable",
      "reading",
      "books",
      "NYT"
    ],
    "contents": "\nHey folks! I’m back again, so soon after my last post. Like I\nmentioned in my last post, I’m going to start posting more, shorter,\nslightly less-polished articles, and this is the start.\nToday I’m looking at New York Times bestselling authors. Tidy Tuesday\nposted\nthese data several months ago, and I was interested in exploring it\na bit. Around the same time I started seeing a lot more folks using the\n{reactable} package for making interactive tables in R. So I figured it\nmight be a good opportunity to test it out on the NYT data.\nOut of the spirit of shorter, less-polished posting, along with the\nfact that I’m definitely not a {reactable} expert, I’m not going to be\nmake this a step-by-step tutorial on how to use the package and make the\ntable. Rather, I’ll just drop all the code and you can see the finished\nproduct.\nFirst, the preliminaries of loading packages, data, and requisite\ndata cleaning.\n\n\n# load packages\nlibrary(tidyverse)\nlibrary(reactable)\n\n# load data\nnyt_titles <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-10/nyt_titles.tsv')\n\nnyt_full <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-10/nyt_full.tsv')\n\n\n\n\n# initial data cleaning\ntop_authors <- nyt_full %>% \n  filter(rank == 1) %>% \n  count(author) %>% \n  arrange(desc(n))\n\nbooks_at_one <- nyt_full %>% \n  filter(rank == 1) %>% \n  group_by(author, title) %>% \n  summarise(\n    weeks_at_one = n(), \n    week_start = min(week), \n    week_end = max(week)\n  ) %>% \n  arrange(desc(weeks_at_one))\n\n\nNow, the {reactable}!\n\n\nreactable(\n  top_authors,\n  filterable = T,\n  columns = list(\n    author = colDef(name = \"Author\"), \n    n = colDef(name = \"Weeks at #1\")\n  ), \n  details = function(index) {\n    sec_lvl = books_at_one[books_at_one$author == top_authors$author[index], ] \n    reactable(\n      sec_lvl, \n      columns = list(\n        author = colDef(name = \"Author\"), \n        title = colDef(name = \"Book\"), \n        weeks_at_one = colDef(name = \"Weeks at #1\"), \n        week_start = colDef(name = \"Start Week\"), \n        week_end = colDef(name = \"End Week\")\n      )\n    )\n  }\n)\n\n\n\nAnd voila! An interactive, drill-down table of the authors with the\nmost weeks atop the NYT bestseller list, complete with the ability to\nsearch authors and view which books were at number one for them.\nThis is it for now. I’ll be back soon with another short post!\nCheers, everyone!\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-08T20:04:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-06-quick-updates-and-musings/",
    "title": "Quick updates and musings",
    "description": "A brief note about where I've been and where I might be going.",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2022-08-06",
    "categories": [
      "no code"
    ],
    "contents": "\nI know it’s been ten thousand years since I posted (ok not really\nthat long, but still pretty long), but I wanted to get back on here and\nrenew my intentions of actually posting on here semi-regularly. The last\nyear or so has been decently bonkers with my daughter, but I really do\nwant to make time for this. Blogging and data exploration is something\nthat really has brought me a lot of joy.\nI guess the reason that I’m writing this now (and hopefully pushing\nit live before Liv wakes up from her nap) is because I’ve been seeing SO\nMUCH about the new Quarto platform from RStudio (soon to be Posit?). I\nwent down a rabbit hole for a few hours this week of trying to figure\nout converting my website from distill to the new Quarto system or\ncreating a new separate “musings” style blog using Quarto. But at the\nend of the day I realized that there is plenty of time for that, and I\ndon’t need to rush into it now - the more helpful and productive\npractice is actually just making use of the blog I have now.\nSo here we are, getting back into it.\nThis is my first new post (albeit not a real post), and from here on\nout, I’d like to mainly do small pieces that don’t necessarily have to\nbe super well-thought out or composed. The perfectionism is what keeps\nme from posting so much of the time, and I want to start to free myself\nfrom that. Occasionally I’m sure I’ll do longer deep dives and\nwalk-throughs, but for now I just want to continue to take the chance to\nwrite, to explore data that interests me without the self-expectations\nof it being perfectly insightful. Consistency and progress are my\ngoals.\nI’m committing here now that the next post I do will be soon, and it\nwill be just a small interactive table showing NYT best sellers.\nTidyTuesday released a data set on NYT best seller lists, and I played\naround with it a few months ago. I already have the code and and the\ntable basically ready, so now I just need to put it in post form and\npublish it.\nBut I wanted to do this post first, as a way to re-commit myself to\nthis. We’ll see where it takes me, and I sincerely appreciate the\nat-most five people who will read this and follow along.\nCheers until next time!\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-08T20:04:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-07-analysis-of-major-marathon-winners/",
    "title": "Analysis of Major Marathon Winners",
    "description": "A brief foray into winners of world major marathons, 1970-2018.",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2022-02-07",
    "categories": [
      "R",
      "running",
      "marathons"
    ],
    "contents": "\nWell folks, it’s hard to believe that it’s been 5 months since I’ve\nlast posted, and close to a year since I’ve posted anything with any\ntype of actual data analysis. C’est la vie with an infant in a\npandemic.\nI have been itching to get back to posting more semi-regularly\nthough, and to take steps towards making that happen, I decided to just\nstart small and get something out there. In that vein, today’s post will\nbe an analysis of the winners of world major marathons from the 1970s\nuntil 2018. I found a small data set on data.world of the names,\nnationalities, and times of the winners of the world’s biggest\nmarathons, and instead of being disappointed in the lack of data (age,\nsplits, more runners, what have you), I decided to take it as a chance\nto do a simple analysis on what was there.\n\n\n\n\n\n\nThere are six annual marathons that are considered “world major\nmarathons” and those are in Berlin, Boston, Chicago, London, New York\nCity, and Tokyo. Hundreds of elite runners from all over the world\ncompete to win these races every year, in addition to the thousands of\nrecreational runners looking to break personal bests and push their\nlimits. The data I’m working with today only contain the winners of\nthese marathons (both male and female), from 1970ish to 2018 (though\nthere’s data going back to 1897 for the Boston marathon. Like I said,\nthe data are pretty sparse - they only have name, nationality, gender,\nand time (in addition to the year and marathon won).\nI’ll start off by looking at some simple descriptive angles of the\ndata - how many individual runners and nationalities are represented in\nthe data, and how does it differ by marathon and gender.\n\n\n\nOverall, there are 316 distinct runners and 38 distinct nationalities\nrepresented in the 514 race results in the data. Honestly that’s\nconsiderably more nationalities than I thought would be represented. My\nprior was that East Africans (Kenya, Ethiopia) would make up the lion’s\nshare, with maybe 10 or 15 other countries popping up here and there -\nshows how much I know.\nNow I’ll do this cut by gender and marathon, in the tables below\n(shout out to the {gt} package!). Here’s how it looks by\ngender:\n\n\nGender\n      Distinct Races\n      Distinct Runners\n      Distinct Nationalities\n      Runners %\n      Nats %\n    Female\n233\n119\n25\n51.07%\n10.73%Male\n303\n196\n32\n64.69%\n10.56%\n\nFrom this table, it looks like there are both more distinct winners\nand more nationalities represented among winning men than women. As a\npercentage of races represented, there are more distinct winners among\nmen than women - there’s a distinct winner in ~64% of races for men, and\nonly a distinct winner in ~51% of races for women. For nationalities,\nthough it looks like there are equal ratios of distinct nationalities to\nraces for men and women (distinct nationalities in ~10% of races).\nNow let’s look at the same table, only cut by marathon instead of\ngender.\n\n\nMarathon\n      Distinct Races\n      Distinct Runners\n      Distinct Nationalities\n      Runners %\n      Nats %\n    Berlin\n88\n68\n18\n77.27%\n20.45%Boston\n175\n124\n23\n70.86%\n13.14%Chicago\n80\n63\n15\n78.75%\n18.75%London\n77\n53\n16\n68.83%\n20.78%NYC\n92\n63\n20\n68.48%\n21.74%Tokyo\n24\n22\n6\n91.67%\n25.00%\n\nKeeping in mind that this table contains races for both men and\nwomen, it looks like Tokyo has distinct winners in a higher percentage\nof races than the other majors - but that’s probably just because there\nare way fewer races. Aside from Tokyo, it looks like Berlin and Chicago\nhave higher rates of distinct winners than Boston, London, and NYC. In\nterms of distinct nationalities, it looks like though it has a slightly\nhigher rate of distinct winners than London or NYC, it has a slightly\nlower rate of distinct nationalities represented among winners.\nVery fascinating.\nAfter these descriptive angles, I’ll move more into an analysis of\nhow winning race times have changed throughout the years. There are a\nfew aspects I’ll look at, and I’ll look for both men and women, and\nacross marathons.\nThe first thing I’ll look at is winning times by marathon across\ntime, from 1980 to 2018. The data for the Boston marathon start in the\n1890s and start years for the others are scattered throughout the 1970s\nand 80s (except for Tokyo, which I guess just became a major marathon\nrecently?), so I figured I’d just start at 1980 to make it cleaner. The\nfirst figure shows men’s winning times.\n\n\n\nThe trajectory of winning times for men is surprisingly different\nacross the major marathons. What I see in this chart is that Berlin and\nLondon (and Tokyo, but harder to compare) have markedly faster courses\nand winning times have improved pretty steadily since 1980. So if you’re\nlooking to set a marathon PR in a major race, maybe choose one of them.\nBoston and Chicago, along with NYC to a lesser extent, seem to have more\nvariably slow courses. This checks out with anecdotal evidence I’ve\nheard about these races, what with Heartbreak Hill in Boston and the\nwind in Chicago. Winning times in these cities haven’t progressed in\nnearly the same way, and are even getting slower in some recent years.\nThat could be due to the field of runners, or the conditions on race\nday, or a number of things I imagine.\nLet’s look at the same figure for women now:\n\n\n\nWe see similar results to men for the women’s winning times across\nmost marathons. Berlin and Tokyo tend to be faster with more sharply\nimproving times, while Boston and NYC tend to be slower with less\nmovement on winning times. Chicago and London seem to have different\ntrajectories for women than men, though. For women, Chicago looks to\nhave more improvement in winning times, and London seems to have more\nstagnation, while it’s the opposite for men.\nTo summarize these charts, I’ll now look at the fastest winning times\nfor each marathon for both men and women.\n\n\n\nFirst, we’ll look for men:\n\n\nMarathon\n      Winning Time\n      Year\n      Winner\n      Nationality\n    Berlin\n02:02:57\n2014\nDennis Kimetto\nKenyaBoston\n02:03:02\n2011\nGeoffrey Mutai\nKenyaLondon\n02:03:05\n2016\nEliud Kipchoge\nKenyaChicago\n02:03:45\n2013\nDennis Kimetto\nKenyaTokyo\n02:03:58\n2017\nWilson Kipsang\nKenyaNYC\n02:05:06\n2011\nGeoffrey Mutai\nKenya\n\nThe table above shows the fastest winning times for each marathon for\nmen, along with who ran them in what year, and what their nationality\nis. First of all, it seems that Berlin does indeed have the fastest\ncourse of the major races, with the fastest time being 2:02:57, run by\nDennis Kimetto in 2014. Kimetto also holds the fastest time in Chicago\nwith 2:03:45. Geoffrey Mutai also shows up twice on this list, with the\nfastest times in Boston and NYC. Eliud Kipchoge and Wilson Kipsang round\nout the fastest times in London and Tokyo, respectively. Another pretty\nnoticeable feat of this table? All of the fastest times at men’s major\nmarathons are held by Kenyan runners. That’s a little more what I was\nexpecting, as I pontificated earlier.\nIt’s also worth noting that these data stop in 2018, and I’m pretty\nsure there has been some movement in these fastest times. If I remember\nright without looking it up, Eliud Kipchoge set the new world record in\nBerlin or London in 2019, with an official time of 2:01:xx. Absolutely\nridiculous.\nNow let’s look at the same thing for women:\n\n\nMarathon\n      Winning Time\n      Year\n      Winner\n      Nationality\n    London\n02:15:25\n2003\nPaula Radcliffe\nUnited KingdomChicago\n02:17:18\n2002\nPaula Radcliffe\nUnited KingdomBoston\n02:18:57\n2014\nRita Jeptoo\nKenyaBerlin\n02:19:12\n2005\nNoguchi\nJapanTokyo\n02:19:47\n2017\nSarah Chepchirchir\nKenyaNYC\n02:22:31\n2003\nMargaret Okayo\nKenya\n\nThe first thing I notice on this table is the bigger spread of\nfastest winning times across marathons - there’s over a seven minute\ndifference between the fastest and slowest best winning time, while\nthere’s only a barely over two minute difference for men. Very\ninteresting. I’m also struck by the nationalities represented with the\nUK and Japan. Anyways, Paula Radcliffe holds the two fastest times, in\nLondon and Chicago, I’m also fairly certain Radcliffe’s record was\nbroken since 2018, but I could be wrong on that one.\nWell, I could probably look at more from the data, but I feel like\nwe’ve gotten a pretty good look in here considering what’s available. I\ndo wish there were more fields, like DOB or age during the win, or more\nrunners, or something, but it’s also nice to have something a bit\nsimpler to look at. While I was searching for data I also found some\nfiles with all the Boston marathon finishers since the 1890s- I might\ntry to look into those data and see what’s there and whether I could\nwrite something up on it. Could be fun to look at distributions from\nmore runners and a lot more years. Stay tuned!\n\n\n\n",
    "preview": "posts/2022-02-07-analysis-of-major-marathon-winners/analysis-of-major-marathon-winners_files/figure-html5/time-across-years-men-1.png",
    "last_modified": "2023-04-08T20:04:57-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-09-15-notes-on-the-2020-poverty-statistics-release/",
    "title": "Notes on the 2020 poverty statistics release",
    "description": "Poverty is down from 2019 to 2020 (by the SPM) - musings on the newest batch of data on poverty and income from the Census Bureau",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2021-09-15",
    "categories": [],
    "contents": "\nOn September 14, 2021, the Census Bureau released it’s newest batch of data on poverty, income, and health insurance coverage for the year 2020. This has been a much anticipated release for poverty researchers and social scientist, as I’m sure most of you will remember that there was a pretty big global happening that upended the economy in 2020 and left everyone wondering just how bad things actually got for folks. (Quick aside: In a normal year I would absolutely have been one of those people that was anticipating the release, but this year I happen to be a new father that’s just doing his best to get by everyday, so I haven’t exactly been paying attention to when big poverty data releases are happening. I happened to see this on one of my monthly forays into the Twitterverse the day it was released, which is why I’m here now.)\nAnyways, the new poverty numbers are really fascinating and meaningful and I thought that I would do a quick post to talk about them. This will be pretty short, as I want it to be decently timely when people read it (on the off chance that anybody actually reads this).\nThe top line number that really is a big deal is that poverty actually decreased by more than 2.5 percentage points overall from 11.8% in 2019 to 9.1% in 2020. Not only that, every subgroup measured experienced a reduction in poverty.\nTake a second to let that sink in. During one of the worst recessions of the last century, in the midst of a global pandemic, poverty actually fell. That’s incredible. Let’s dive into that a little more:\nThe measure that the Census Bureau used to calculate the decrease is the Supplemental Poverty Measure (SPM), which is different than the official poverty measure that’s used to determine means-tested benefits and such. The SPM is widely considered to be a much better measure of poverty because it uses a way more reasonable measure of cost of living in its calculations, and it takes into account common household expenses like rent, taxes, medical expenses, child care, and the like. It also counts government benefits and tax credits as income, which is critical for this story.\nAlmost by nature, poverty gets worse with recessions - they’re basically part and parcel. So why did poverty actually go down in the most severe recession that we’ve seen in decades - not just stay the same, but decrease?\nThere’s actually a really simple solution here - the government gave people money. Not just people experiencing poverty, but everybody. Not only did the federal government send three rounds of stimulus checks, but it majorly beefed up unemployment and food and rental assistance and other benefits for people who desparately needed it. While the official poverty rate doesn’t capture any of these benefits (it’s based on pre-tax and transfer income and increased by several points), the SPM takes these benefits into account, and it’s clear that they had a massively positive impact. Not only did it make up for the massive loss of income that so many people experienced during COVID, it actually lifted millions out of poverty.\nThere’s so much I could say about this, but I really want to keep this short. I’ll probably end up writing more about this in future posts, as well as including some pretty plots. I’ll end this one by saying this though: The question isn’t whether we know how to solve poverty - these numbers make it clear that we know exactly how to solve poverty. The question is whether we’re only willing to do what it takes to solve poverty in a once-in-a-lifetime pandemic.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-08T20:04:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-12-plug-for-the-correlates-of-state-policy-shiny-app/",
    "title": "Plug for the Correlates of State Policy Shiny App",
    "description": "A quick shout out to a really cool and valuable tool for social scientists",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2021-04-12",
    "categories": [
      "state policy",
      "no code"
    ],
    "contents": "\nI know, I know, this is my second short, no-code post in a row, but I stumbled across a really cool resource that I wanted to share with y’all.\nA few years ago a few political scientists released a massive treasure trove compendium of data on historical and modern state policy decisions for a huge amount of policies and programs across a wide variety of domains, along with associated demographic and political characteristics for state-years from 1900 to current, dubbed the Correlates of State Policy. As I was just about to start my Master of Public Policy program when I found it, of course I knew what a fantastic resource this was, and I was super pumped to dig into it. After exploring the website and associated files a bit, though, it proved to be way too clunky and overwhelming to really take the time to get to know and work with - thousands of variables and only a standard PDF codebook from what I remember. Not at all something I had time or energy to dive into and make sense of. So I forgot about it for a few years.\nBut this week I was searching the webz for fun data ideas for the blog and stumbled across a marvelous development in the Correlates of State Policy data world - a Shiny app that allows users to subset and download the data and explore visually some of the features. Cue the fireworks! It’s super cool that they have some analytics and visual capacity built in to explore the data online - that’s rad. But the thing I got super pumped about was the tool to filter and view the relevant variables and associated descriptions and download the data according to different policy domains and topics. This makes downloading the data soooo much easier than just grabbing the raw full data file and all three thousand columns. Not only can I pick the topics I want to download data for, but I can see how many variables it’ll return, and I can filter to specific states and years, to boot.\nThis is an amazing resource for social scientists or policy wonks or anyone doing broad-scale (quantitative) policy analysis. The data are super extensive and this is a really user friendly way to explore and download all that they have to offer. I will say that it’s probably wise to have the codebook pulled up as you’re viewing the variables for different policy domains. The coverage in terms of which years the variables are available for vary widely across the sources that the authors pull the data from, even within the same domain and it’s not always intuitive what data cover what years from just looking at the Shiny app. That’s really my only word of caution though. Everyone should check out this incredible resource. (That and the fact that they don’t link to the code used to develop the app.)\nIn addition to it being a great resource for academics and researchers, this is just an example of a really useful and functional Shiny app. Not only do they provide interactive graphics and user-guided analysis, but they make the underlying data really accessible, which is so clutch when the data are as unwieldy as they are in this case.\nWell, that’s all for now. I’m definitely thinking about my next substantive post for the blog, but am at a slight crossroads with what I want to focus on and spend my time doing. Do I go with more simple exploratory type analysis or do I augment my causal inference toolbox with mini causal type research questions and models or do I explore new machine learning territories that I don’t really know at all? Do I stick with data on policy topics that I work with often, or do I branch out to other data that are less serious and more for the fun of it? Do I build more shiny apps and explain them here or do I focus more on the guided analysis and insight posts that I’ve typically done? All really good things to spend my time on, and all things I want to spend time on, I just need to decide what to prioritize first and start digging in.\nUntil next time!\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-08T20:04:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-05-new-shiny-app-on-world-coffee-ratings/",
    "title": "New Shiny App on World Coffee Ratings!",
    "description": "Short plug for my new Shiny app on world coffee ratings",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2021-04-05",
    "categories": [
      "shiny",
      "R",
      "coffee"
    ],
    "contents": "\nHey hey hey, folks. Hope everyone is doing swell. I wanted to take just a few minutes to do a small post about a fun little Shiny web app I just created on world coffee ratings.\nCoffee, especially the specialty coffee scene, is near and dear to my heart. Back in college and for a brief period during my first master’s I worked at a local specialty coffee shop in Fayetteville, Arkansas called Onyx Coffee Lab, and it introduced me to a world I will forever appreciate. For those that have never delved into this world, coffee is a product that is similar to wine in that there is a whole world of producers, varietals, processing methods, brewing/production methods, and even competitions that accompany it. It’s not just caffeine and dark roasts, Folgers and Starbucks - there is an entire industry dedicated to finding, procuring, roasting, and brewing the best coffees in the world.\nThis Shiny app (and post) is an homage to that world.\nI was hunting through old TidyTuesday data archives and came across a week with world coffee ratings as the featured data, and I jumped on it. It’s honestly a pretty simple web app, but I think it’s really fun. A user can select from any coffee in the database and it will pull up a polar coordinate set of its ratings along a few different dimensions, including acidity, balance, sweetness, uniformity of beans, etc, all categories that determine how good a coffee is.\n\n\n\n\nOne of the fun things that I wanted to play around with was the ggiraph package that allows you to create interactive elements in ggplot figures. That’s how I got the tooltip that shows the ratings when you hover over different rating segments. You can find the code that I used to build the app here.\nThere are a few things that I wish were different - for one, there was no unique identifier or name for the coffees in the database, and there were lots of duplicate fields for different coffees, so I had to concatenate way too many fields (clearly) to get the identifier in the drop down filter. I’m considering going back and adding filters for all of those elements individually to make it a bit easier to handle.\nAnyways, just wanted to share the latest thing I’ve been working on. Check out the app and I’ll be back soon! The next post I’m thinking about is exploring the tidymodels universe with a treasure trove of new COVID-19 data that the CDC just published. Stay tuned!\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-08T20:04:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-13-analyzing-my-running-performance-using-strava-data/",
    "title": "Analyzing my 2020 running performance",
    "description": "Exploring my year in running using data from the Strava app",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2021-03-27",
    "categories": [
      "R",
      "running",
      "strava",
      "leaflet"
    ],
    "contents": "\nHello, folks! In this post I’ll be looking at my running performance over the last year by analyzing my running data from the Strava app. Before I start that, here’s a bit of background on how I’ve experienced a new-found love of running over the last year and what led to this in the first place.\nI believe I’m not the first to point out that this year has been a weird one. Societally, it’s been mostly pretty shitty (until the second week in November when things started looking up a bit). The pandemic has upended every facet of life, hundreds of thousands of people have died needlessly, there were kiler hornets, and I’m sure I’m forgetting about ten other big terrible things that happened (feel free point them out if you feel so inclined). Personally, this year has been a mixed bag for my wife and me. We’ve felt all the terrible things going on in the world at large, and our lives look so much different than we thought they would. While we’ve had our fair share of pain and grief caused by the pandemic, we were left in a relatively good spot - we both work from home in stable jobs, we were able to buy a house in a neighborhood that we love, and we were able to plan to start our family. We don’t take that for granted at all, and we truly grieve with those whose lives have been completely devastated over the last year.\nWhen the pandemic shut everything down for us starting in mid March, one of the more noticeable things to stop for us was working out at the gym. In the wake of that, my wife and I decided to start running more. At the time we were living on the upper floor in a pretty small duplex so we didn’t have a whole lot of space to do anything more than very tame bodyweight workouts inside, and we knew that we needed a physical outlet to get away from the stress of the pandemic and of me finishing my final semester of grad school without the confines of our small living space.\nEnter running. I’ve never been a complete stranger to running over the course of my life, but I’ve also never really embraced it. There have been various fits and starts of running more frequently, but it’s never been what I would call sustained. Mar had periods of more dedicated, sustainable running over the last decade or so, even training for and running a half-marathon. She could always handle distance better than I could, but I liked running faster. Since we moved to Minneapolis in 2017 up until last March, Mar and I ran an average of once every week or two probably, never more than 2 or 3 miles at a time. We liked to run around our first neighborhood here, and there were a few months where we would go run a 3 ish mile loop around a nice lake every weekend. But I’ve never exactly loved running. I’ve done it because I know aerobic exercise is good for me and because it’s a good way for Mar and I to workout together every so often, but I’ve always liked lifting and bodyweights and sprint-type workouts a lot better. Until last March, the farthest I think I’d ever run was five miles tops, probably less. I could do a mile decently fast for a normal non-runner (I think I ran a sub-six minute mile a few times in college), but I never liked running longer distances than that.\nFast forward to now, close to a year since we started that journey, I’ve run almost 1,000 miles since April, the majority of it coming in the last five or so months. Mar and I started running as a way of coping once things shutdown, but somewhere along the way it became a new thing for me, something that I genuinely started to love. We started out slow, running a mile and half or two most days, with a long run of 5.5 or 6 on Friday. Our first week of 20 miles, probably in late April or early May, was a big milestone, as was the time we ran 9 miles (thrice around our normal Friday 3 mile lake loop). June and July brought a bit of a slow-down again, with some minor injuries and tweaks, moving into our new house, and the heat of the summer. But in September I bought a new pair of running shoes (my first in probably six years) and started to get back into it again, and it was the start of a real thing for me. Mar and I still ran together from time to time, but she was in the throes of first trimester nausea, and I was increasing my mileage to 25 to 30 miles a week. I did a ten mile run in mid October at a decent pace, and that might have been what sealed it for me. In November I set out to do another 10-miler but through a weird series of events I ended up running my first half-marathon - my time was 1:50 ish, which while not competitive, is pretty good for someone who started running in earnest in March and had never run a half before. Since then I’ve been doing 30-35 mile weeks with some rest weeks here and there, and I’ve really gotten into it. I found my first running podcasts around a month ago, and started to plan my training more strategically based on what I was learning, easing my pace, incorporating strides, and dedicating one run every week or two to be more of an intervals workout.\nThe throes of winter cold and snow have descended upon Minneapolis by now, so I imagine my training will slow down a bit until mid-March or so, but all of this to say that over the last 9 months I’ve found a new love of running that will be with me for a long time I think. Once the winter thaw comes I’d like to start training for my first marathon (even if it is just by myself, but hopefully I’ll be able to find a race by then), and who knows what will come after that.\nI’ve also started to combine this new-found love of running with my love of data. In October I started downloading my running data from Strava and it’s been fun to explore it since. Today’s post will be a more formal dive into my running performance over the last year, primarily to help me see how far I’ve come and allow me to be proud of myself (something I have a really hard time with most days). Here we go! ***\nSo! To start. I’ll have two broad sections to this analysis. First I’ll do a basic descriptive look at my running performance in 2020, cutting the data by week, month, day of the week, etc. Second, I’ll make an interactive map of all my runs using leaflet so we can visualize where I run and what my typical routes look like.\nI downloaded my Strava account data from the online site, which resulted in getting a big zipped folder of a lot of different files. I know I published this post in February of 2021, but I decided to just analyze the data from 2020 to keep it simple. There are two pieces of that I incorporate here. The first is an overview file of all my activities I’ve recorded on Strava, including the name, date, time, distance, pace, etc. The second is a folder of all of the GPS location data for all of the activities, in GPX format. I’ll get back to the GPX location files here in a bit, and start with the raw activities file.\nThis code chunk loads my packages using pacman and imports and cleans the data from the activities file. Most of the cleaning is converting paces and distances into the units I generally use (miles instead of kilometers, pace in minutes per mile, etc.). Also, shoutout to the lubridate package for making the date-time parsing so painless.\n\n\n# install.packages(\"leaflet\")\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(gtools)\nlibrary(devtools)\nlibrary(leaflet)\nlibrary(gt)\nlibrary(sp)\nlibrary(maptools)\nlibrary(here)\n\n### load and clean activity summaries - no location data\nstrava <- read_excel(here(\"_posts\", \n\"2020-12-13-analyzing-my-running-performance-using-strava-data\", \n\"strava_activities.xlsx\")) %>% \n  clean_names() %>% \n  mutate(act_date = mdy_hms(activity_date), \n         act_date_time = (act_date - hours(5)),\n         month = month(act_date_time, label = T), \n         day = day(act_date_time), \n         year = year(act_date_time), \n         mdy = format(act_date_time, format = \"%m-%d-%Y\"),\n         week = floor_date(act_date_time, unit = \"week\"),\n         wkday = wday(act_date_time, label = T),\n         time = format(act_date_time, \"%H:%M:%S\"), \n         move_time = moving_time/60, \n         distance = distance_7/1.609, \n         avg_pace = move_time/distance, \n         avg_speed = average_speed/1.609, \n         max_speed = max_speed/1.609, \n         shoe = ifelse(activity_type == \"Run\" &\n                         act_date_time >= as.Date(\"2020-09-08\") & \n                         act_date_time <= as.Date(\"2020-11-15\"), \"Ghosts\", \n                       ifelse(activity_type == \"Run\" & \n                                act_date_time <= as.Date(\"2020-09-08\"), \"North Face faithfuls\", activity_gear)), \n         multiplier = distance/avg_pace, \n         performance = move_time*multiplier) %>% \n  select(act_date_time, month, day, year, mdy, week, wkday, \n         time, move_time, distance, avg_pace, avg_speed, \n         max_speed, multiplier, performance, shoe, everything())\n\n\n\nOne thing I want to point out is the creation of the multiplier and performance variables towards the bottom of the chunk. I wanted a way to compare my runs in a more standardized way that didn’t just use distance or average pace as the metric. Those are instructive indicators, but only to a certain point; I can run long distances, but if my pace isn’t where I want it to be then it doesn’t mean as much. Likewise, I can run at a fast pace, but if I can’t sustain it for longer distances, it also doesn’t mean as much. The performance metric I thought of calculates the distance divided by pace as a multiplier for how long a run is. If I run 60ish minutes for 8 miles at 8 minutes a mile, I’ll get a multiplier of around 1, which leaves my performance at ~60. As opposed to if I run 60 minutes for 10 miles at 6 minutes a mile (which I absolutely can not do), the multiplier would be 1.4ish, and the performance would be 85ish. More on this later.\nFirst things first, I want to do a basic look at my running mileage over time. I’ll start by doing a look by month, then by week.\nHere’s the monthly look.\n\n\nstrava %>% \n  filter(activity_type == \"Run\") %>% \n  group_by(month) %>% \n  summarize(mileage = sum(distance, na.rm = T)) %>% \n  ggplot(aes(x = month, y = mileage)) +\n  geom_col(fill = \"#d98004\") +\n  scale_y_continuous(limits = c(0,150), breaks = seq(0,150, by = 30)) +\n  xlab(\" \") +\n  ylab(\"Total Mileage\") +\n  labs(title = \"Cody's Monthly Running Mileage \\n\",\n       caption = \"Source: Strava\") +\n  theme_classic() +\n  theme(\n    plot.title.position = \"plot\", \n    plot.caption.position = \"plot\",\n    plot.title = element_text(size = 13)\n  )\n\n\n\n\nNow for weekly:\n\n\nstrava %>% \n  filter(activity_type == \"Run\") %>% \n  group_by(week) %>% \n  summarize(mileage = sum(distance, na.rm = T)) %>% \n  ggplot(aes(x = week, y = mileage)) +\n  geom_col(fill = \"#d98004\") +\n  scale_y_continuous(limits = c(0,50), breaks = seq(0,50, by = 10)) +\n  xlab(\" \") +\n  ylab(\"Total Mileage\") +\n  labs(title = \"Cody's Weekly Running Mileage \\n\",\n       caption = \"Source: Strava\") +\n  theme_classic() +\n  theme(\n    plot.title.position = \"plot\", \n    plot.caption.position = \"plot\",\n    plot.title = element_text(size = 13)\n  )\n\n\n\n\nThese two graphs show more or less what I explained earlier. My mileage increased throughout spring and early summer 2020, plummeted during the mid-summer, and then started to pick back up again in the fall and through the end of the year to its highest levels. It’s interesting how much the weekly look reveals that the monthly hides - even in the valley of the summer and the peaks of the fall, there’s still a lot of ups and downs. Some of that variation is the ebb and flow of my work and recovery weeks, and some of it is the weather getting harder with snow and such. Though you can’t see it, this bouncy pattern has continued into the new year. One thing I’d like to work on being more consistent with my weekly mileage - trying to ease into smoother increases and decreases in mileage rather than the stark up and downs we see here.\nObviously I knew approximately what these charts would look like. One thing I’m not as sure of how my runs break down on days of the week and time of day, both in terms of the number of runs and total mileage. This next graphs look at both the number of runs and total mileage by day of the week.\n\n\nstrava %>% \n  group_by(wkday) %>% \n  count() %>% \n  ggplot(aes(x = wkday, y = n)) +\n  geom_col(fill = \"#0c0757\") +\n  scale_y_continuous(limits = c(0,100), breaks = seq(0,100, by = 20)) +\n  xlab(\" \") +\n  ylab(\"Total Runs\") +\n  labs(title = \"Cody's Top Running Days by Total Runs \\n\",\n       caption = \"Source: Strava\") +\n  theme_classic() +\n  theme(\n    plot.title.position = \"plot\", \n    plot.caption.position = \"plot\",\n    plot.title = element_text(size = 13)\n  )\n\n\n\nstrava %>% \n  group_by(wkday) %>% \n  summarize(mileage = sum(distance, na.rm = T)) %>% \n  ggplot(aes(x = wkday, y = mileage)) +\n  geom_col(fill = \"#0d0263\") +\n  scale_y_continuous(limits = c(0,200), breaks = seq(0,200, by = 50)) +\n  xlab(\" \") +\n  ylab(\"Total Mileage\") +\n  labs(title = \"Cody's Top Running Days by Mileage \\n\",\n       caption = \"Source: Strava\") +\n  theme_classic() +\n  theme(\n    plot.title.position = \"plot\", \n    plot.caption.position = \"plot\",\n    plot.title = element_text(size = 13)\n  )\n\n\n\n\nThis is new to me! Wednesdays were my biggest days for running overall and second biggest day for mileage - I wasn’t expecting that. I was thinking Fridays might be my biggest running day, but I guess not. Saturdays were my biggest days mileage-wise, which isn’t a surprise as that’s mroe a long run day for most runners (including me), but Wednesdays got close to it. The pattern seems to have been more runs with lower mileage on Wednesdays and fewer runs with higher mileage on Saturdays. So interesting.\nNow I want to go beyond the mileage piece and look more at the performance aspect that I touched on earlier. While it’s insightful to look at distance, it’s also instructive to think about my efficiency and speed as a runner. Enter the calculation I talked about above for performance. There are a few other caveats that I want to put in here though. While the calculation I came up with (imho) is pretty good and standardizes a lot of a run, it can fall victim to extremes, either mileage wise or speed wise. Middle distance and smooth pace is more reliable. So I’ll take it with a bit of a grain of salt, and you probably should, too.\nWhat I’ll do with the performance metric is chart it across month and week, probably as a median to do a better job of smoothing outliers during those periods. I had half a mind to do a simple regression to try to pick out the best predictors of my performance but honestly, the time series nature of it would make any significant or interesting results more or less useless. I don’t do time series analysis very much and it’s hard to do right so I figure I won’t wade into it this time. (Side note, I do want to get into a bit more modelling in this blog space, but I don’t think these are the right data. Maybe I could do that for my running when I have more months’ worth of data and it’s such a clearly linear trend of increased mileage and performance.)\nHere’s a look at my median performance by month.\n\n\nstrava %>% \n  filter(activity_type == \"Run\") %>% \n  group_by(month) %>% \n  summarize(performance = median(performance, na.rm = T)) %>% \n  ggplot(aes(x = month, y = performance)) +\n  geom_col(fill = \"#d98004\") +\n  scale_y_continuous(limits = c(0,30), breaks = seq(0,30, by = 10)) +\n  xlab(\" \") +\n  ylab(\"Median Performance\") +\n  labs(title = \"Cody's Monthly Running Performance \\n\",\n       caption = \"Source: Strava\") +\n  theme_classic() +\n  theme(\n    plot.title.position = \"plot\", \n    plot.caption.position = \"plot\",\n    plot.title = element_text(size = 13)\n  )\n\n\n\n\nAfter seeing the grpahs for mileage, it shouldn’t be a surprise that my performance goes up continually once the fall settles in. Here’s the same look by week for a more granular look.\n\n\nstrava %>% \n  filter(activity_type == \"Run\") %>% \n  group_by(week) %>% \n  summarize(performance = median(performance, na.rm = T)) %>% \n  ggplot(aes(x = week, y = performance)) +\n  geom_col(fill = \"#d98004\") +\n  scale_y_continuous(limits = c(0,45), breaks = seq(0,45, by = 15)) +\n  xlab(\" \") +\n  ylab(\"Median Performance\") +\n  labs(title = \"Cody's Weekly Running Performance \\n\",\n       caption = \"Source: Strava\") +\n  theme_classic() +\n  theme(\n    plot.title.position = \"plot\", \n    plot.caption.position = \"plot\",\n    plot.title = element_text(size = 13)\n  )\n\n\n\n\nThis also checks out - not sure when the week is that I had the massive median performance. It could have been mid November when I ran my first half marathon. That would make sense.\nNow what I want to do is a map of my routes. I love maps and making them, so this is something I’m excited about it. There are a lot of things I could choose to do with this, like color the lines of my route by performance or by the date, but what I think I’ll do is keep things simple and just map the lines themselves. I run a fair few routes, but there are some that I do a lot more than others, so the map should make those lines thicker if I leave the color out of it.\nOne thing I’ll say here is that a lot of the data cleaning for the next section is taken more or less directly from other people’s code on GitHub or StackOverflow. I wouldn’t have been able to do this if a lot of way smarter people than me hadn’t done what I’m doing and been kind enough to share their code online. I’ve never worked with GPX files before, but luckily I was able to adapt code someone wrote for a Strava R package. Fun fact, my computer didn’t seem to want to just download the package, so I was left to scour the source code and bring in what I needed to work with the folder of GPX files. Here’s the setup code, including the functions to parse the GPX data.\n\n\n### function to import and process GPX strava data files for locations\n\nprocess_data <- function(path, old_gpx_format = FALSE) {\n  # Function for processing a Strava gpx file\n  process_gpx <- function(file) {\n    # Parse GPX file and generate R structure representing XML tree\n    pfile <- XML::htmlTreeParse(file = file,\n                                error = function (...) {},\n                                useInternalNodes = TRUE)\n    \n    coords <- XML::xpathSApply(pfile, path = \"//trkpt\", XML::xmlAttrs)\n    # extract the activity type from file name\n    type <- stringr::str_match(file, \".*-(.*).gpx\")[[2]]\n    # Check for empty file.\n    if (length(coords) == 0) return(NULL)\n    # dist_to_prev computation requires that there be at least two coordinates.\n    if (ncol(coords) < 2) return(NULL)\n    \n    lat <- as.numeric(coords[\"lat\", ])\n    lon <- as.numeric(coords[\"lon\", ])\n    \n    if (old_gpx_format == TRUE) {\n      ele <- as.numeric(XML::xpathSApply(pfile, path = \"//trkpt/ele\", XML::xmlValue))\n    }\n    \n    time <- XML::xpathSApply(pfile, path = \"//trkpt/time\", XML::xmlValue)\n    \n    # Put everything in a data frame\n    if (old_gpx_format == TRUE) {\n      result <- data.frame(lat = lat, lon = lon, ele = ele, time = time, type = type)\n    } else {\n      result <- data.frame(lat = lat, lon = lon, time = time, type = type)\n    }\n    result <- result %>%\n      dplyr::mutate(dist_to_prev = c(0, sp::spDists(x = as.matrix(.[, c(\"lon\", \"lat\")]), longlat = TRUE, segments = TRUE)),\n                    cumdist = cumsum(dist_to_prev),\n                    time = as.POSIXct(.$time, tz = \"GMT\", format = \"%Y-%m-%dT%H:%M:%OS\")) %>%\n      dplyr::mutate(time_diff_to_prev = as.numeric(difftime(time, dplyr::lag(time, default = .$time[1]))),\n                    cumtime = cumsum(time_diff_to_prev))\n    result\n  }\n  \n  # Process all the files\n  data <- gtools::mixedsort(list.files(path = path, pattern = \"*.gpx\", full.names = TRUE)) %>%\n    purrr::map_df(process_gpx, .id = \"id\") \n}\n\n# strava location data from GPX files\nstrava_locs <- process_data(here(\"_posts\", \n\"2020-12-13-analyzing-my-running-performance-using-strava-data\", \"activities\"))\n\n\n\nThe next tricky bit after just reading in the GPX files was figuring out how manipulate them into line segments rather than points, so I could map my routes. Luckily, Kyle Walker has a great post on doing just that and created a handy function I was able to use, shown below.\n\n\n### function to clean points data to lines for easier mapping\n\npoints_to_line <- function(data, long, lat, id_field = NULL, sort_field = NULL) {\n  \n  # Convert to SpatialPointsDataFrame\n  coordinates(data) <- c(long, lat)\n  \n  # If there is a sort field...\n  if (!is.null(sort_field)) {\n    if (!is.null(id_field)) {\n      data <- data[order(data[[id_field]], data[[sort_field]]), ]\n    } else {\n      data <- data[order(data[[sort_field]]), ]\n    }\n  }\n  \n  # If there is only one path...\n  if (is.null(id_field)) {\n    \n    lines <- SpatialLines(list(Lines(list(Line(data)), \"id\")))\n    \n    return(lines)\n    \n    # Now, if we have multiple lines...\n  } else if (!is.null(id_field)) {  \n    \n    # Split into a list by ID field\n    paths <- sp::split(data, data[[id_field]])\n    \n    sp_lines <- SpatialLines(list(Lines(list(Line(paths[[1]])), \"line1\")))\n    \n    # I like for loops, what can I say...\n    for (p in 2:length(paths)) {\n      id <- paste0(\"line\", as.character(p))\n      l <- SpatialLines(list(Lines(list(Line(paths[[p]])), id)))\n      sp_lines <- spRbind(sp_lines, l)\n    }\n    \n    return(sp_lines)\n  }\n}\n\n\n\nAfter that, I can convert my points data to lines, below.\n\n\nstrava_locs_clean <- strava_locs %>% \n  as_tibble() %>% \n  clean_names() %>% \n  rename(time_old = time) %>% \n  mutate(act_date = ymd_hms(as.character(time_old)), \n         act_date_time = (time_old - hours(5)),\n         month = month(act_date_time), \n         day = day(act_date_time), \n         year = year(act_date_time), \n         mdy = format(act_date_time, format = \"%m-%d-%Y\"),\n         week = week(act_date_time),\n         wkday = wday(act_date_time, label = T),\n         time = format(act_date_time, \"%H:%M:%S\"))\n\nstrava_lines <- points_to_line(\n  data = strava_locs_clean, \n  long = \"lon\", \n  lat = \"lat\", \n  id_field = \"id\", \n  sort_field = \"cumtime\"\n)\n\nline_match_run <- strava_locs_clean %>% \n  filter(cumtime == 0) %>% \n  left_join(strava, by = \"act_date_time\") %>% \n  arrange(id)\n\n\n\nNow I need to join the spatial lines table (strava_lines) back in with the rest of the fields for the activities to create a spatial lines data frame. This is mainly so I can filter down to just runs and get the hikes and walks out of there. (I also have a few activities that are labelled as runs that are actually when I was playing tennis, so I’ll get those out of there too.) The reason I do subset instead of dplyr’s filter is because the spatial lines data frames don’t work well with tidy verse. I suppose I could have filtered the rows before joining and just done an inner join, but whatever.\n\n\nstrava_sldf <- SpatialLinesDataFrame(strava_lines, line_match_run, match.ID = F)\n\nstrava_run_sldf <- subset(strava_sldf, strava_sldf$activity_type == \"Run\" & \n                            !grepl(\"tennis\", strava_sldf$activity_name, ignore.case = T))\n\n\n\nNow we’re ready to map! My go-to package for mapping is leaflet - so simple and intuitive to build and there are some great features. I won’t build too complicated of a map here, but I’m sure I’ll make more advanced maps in future posts. Here I’m choosing to zoom my map to northeast Minneapolis because that’s where the majority of my runs originate. However, I have run in a few other places and recorded on Strava, like a few trails in some Twin Cities suburbs, and a few in Grand Marais, on the north shore of Lake Superior.\nThe great thing about leaflet maps is that they are interactive, though! So you can pan to different parts of the city to follow routes, and you can zoom in and out to look at routes more closely. You should also zoom out a bit and see if you can find those other routes that aren’t in Minneapolis, too!\nThe lines are darker when there are more routes there, and lighter when there are fewer routes. Zooming in really close to the lines will show you just how many routes are there.\n\n\nmap <- leaflet::leaflet(data = strava_run_sldf) %>% \n  addProviderTiles(providers$CartoDB.Positron) %>%\n  addPolylines(weight = 3) %>% \n  setView(-93.2474, 45.0132, zoom = 12.5)\n\nhtmlwidgets::saveWidget(map, here(\"_posts\", \n\"2020-12-13-analyzing-my-running-performance-using-strava-data\", \"strava_map.html\"))\n\n\n\n\n\n\n\nBut this is all for now. I could do a lot more with this, but honestly, I just want to publish it. I’ve been working on it for several months now and I’ve only just worked up the motivation to finish it. So here it is! Woo!\n\n\n\n",
    "preview": "posts/2020-12-13-analyzing-my-running-performance-using-strava-data/analyzing-my-running-performance-using-strava-data_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2023-04-08T20:04:57-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-12-07-a-true-welcome/",
    "title": "A true welcome",
    "description": "Why I'm here",
    "author": [
      {
        "name": "Cody Tuttle",
        "url": "https://codyrtuttle.netlify.app/"
      }
    ],
    "date": "2020-12-07",
    "categories": [
      "no code",
      "intro"
    ],
    "contents": "\nGreetings, esteemed guest!\nI’m so glad you made it to my site. Whether you’ve known me for a long time and you’re only here because I asked you to check it out, or we’ve never met and you stumbled across this somehow, thanks for being here. I know that there are a lot of other places on the internet you could be and a lot of other things you could be doing with your time, so that you’ve chosen to spend a few minutes here is something I don’t take lightly. Thanks.\nI wanted to take a few minutes to jot down some thoughts about why I started this and what I’m doing here. As you probably figured out by now, this is my site where I blog about data and public policy. I’m a research scientist and policy analyst by trade, and I love working with data - this is my creative outlet to explore the areas that I’m passionate about and get better at my craft. Exploring new data sets about topics that I care about is really exciting to me, because I get to both learn a lot and help people out in the process. One of the ways that I learn best is by following along as people show me how to do something, and that applies to data science especially. I love finding new data and R blogs to follow and explore, particularly when they’re at a skill level that’s close to mine or just slightly more advanced. Hopefully this site can be that kind of a resource to people who are interested in improving their data tool box.\nOld site\nThis site is really a reincarnation of a previous blog that I started back in the summer of 2019. I was on break from my MPP program at the time, only doing an internship and no classes, and I wanted to do something productive with my down time that would help my job prospects once I graduated, either in terms of actual experience and skill-sharpening or just a portfolio to point to. Over the course of the summer I really started to enjoy the process of exploring new data sets and writing about both my process of analaysis and visualization and also the insights and thoughts I was pulling from the analysis. I did circa 10 posts over the course of around three months, most of them going through new data sets in R, but also some posts about Tableau public dashboards I’d made or with my thoughts about a particular policy area that I’d been thinking about a lot. After school started back up again in the fall I wasn’t able to keep it up, but I did do a few more posts during the winter break between fall and spring semesters. Since I graduated in May 2020, I’ve been meaning to pick it back up again, but time the last six months has gotten away from me, with starting a new full-time job, buying a house, finding out that my wife and I are expecting our first child, and general COVID craziness. I also knew that I wanted to create a new site, which would take some planning and investment on top of the time to actually write new posts. My old site, while functional and serviceable, didn’t look that great and it was not at all integrated with the rest of the writing and analysis workflow I’ve developed in RStudio and R Markdown.\nNew site!!!\nEnter this new site! At various points over the last year, I’ve seen posts and resources on various ways to start blogs using R packages, like blogdown and distill, but I wasn’t able to really sit down and start thinking about it until now. I tried starting a site using blogdown and Hugo, and while I’m sure it’s a great tool that works for a lot of folks, I wasn’t able to get it to work. One of the other posts that I’d seen, though, was this one from Tom Mock about starting a blog using distill and Netlify - it worked a lot more seamlessly for me than did the blogdown approach, and now we find ourselves here!\nMostly, I see this new site as a better-looking and easier-to-work-with continuation of the old site in terms of content and purpose. I’m really excited to get back to exploring new data sets and getting better at my analysis and visualization skills, so I imagine that will by main focus for the first stage in this new site. I’d also like to explore new territory with this blog, though. There are a few domains that I’ve been really itching to get into, like machine learning, interacative web graphics using d3 and JavaScript, and I’d love to use this as a way to explore them. I’d also like to create more Shiny web apps and blog about them here. Currently I have one Shiny app that I made for a class, and I’ve been meaning to build more - it’s a super fun platform.\nRight now I’m thinking that the first new post that I do here will be analyzing my Strava app running data. I figured out how to download it a few weeks ago and I’ve had a good time playing around with it. I’m also debating building a Shiny app for it, but that remains to be seen. Other than that, I’m wide open to new data sets and topics.\nWell, folks, that’s all for now. If you want to learn more about me, check out the “About” page on the site. Thanks for stopping by and taking the time to read this - I’m looking forwrad to posting more soon!\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-08T20:04:57-05:00",
    "input_file": {}
  }
]
